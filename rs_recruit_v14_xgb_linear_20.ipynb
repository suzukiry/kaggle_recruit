{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test xgboost with Bayesian Optimsation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thanks to Shinji Suzuki\n",
    "\n",
    "# OS\n",
    "import glob, re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# data science tool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# machine learning\n",
    "from sklearn import *\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# データの読み込み\n",
    "# 事前にcalendar_dateをvisit_dataに変更しています。airとhpgで同じことですが、別名で使用されているようです。\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "}\n",
    "\n",
    "# それぞれのデータをマージするために、まずは、relation用のものをマージします\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how = 'inner', on = ['hpg_store_id'])\n",
    "\n",
    "for df in ['ar', 'hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r:(r['visit_datetime']- r['reserve_datetime']).days, axis = 1)\n",
    "    tmp1 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how = 'inner', on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'dow':[i]*len(unique_stores)}) for i in range(7)], axis =0, ignore_index = True).reset_index(drop = True) \n",
    "\n",
    "#曜日だけでなく、月も追加\n",
    "stores_m = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'month':[i]*len(unique_stores)}) for i in range(1,13)], axis =0, ignore_index = True).reset_index(drop = True)\n",
    "stores = pd.merge(stores_m, stores,on=('air_store_id'), how='left')\n",
    "\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].min().rename(columns = {'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].median().rename(columns = {'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].max().rename(columns = {'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].count().rename(columns = {'visitors':'count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "\n",
    "#曜日だけでなく、ID×月も追加\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].min().rename(columns = {'visitors':'m_min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'m_mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].median().rename(columns = {'visitors':'m_median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].max().rename(columns = {'visitors':'m_max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].count().rename(columns = {'visitors':'m_count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "\n",
    "stores = pd.merge(stores, data['as'], how= \"left\", on = ['air_store_id'])\n",
    "\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/', ' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-', ' ')))\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name' + str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "    stores['air_area_name' + str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "#土日フラグ(day_of_week_1)と、休日前(holi_2)フラグを追加\n",
    "data['hol']['day_of_week_1']= data['hol']['day_of_week'].replace(['Saturday', 'Sunday','Monday','Tuesday','Wednesday','Thursday','Friday'],['1', '1','0','0','0','0','0']).astype('int')\n",
    "data['hol']['holi_2'] = data['hol'][['holiday_flg', 'day_of_week_1']].sum(axis = 1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].apply( lambda x: 0 if x < 1 else 1 )\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].shift(-1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].fillna(1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].astype('int')\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how ='left', on = ['visit_date'])\n",
    "test = pd.merge(data['tes'], data['hol'], how ='left', on = ['visit_date'])\n",
    "\n",
    "#曜日と月でmerge\n",
    "train = pd.merge(train, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "test = pd.merge(test, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "\n",
    "#ID×休日前でのvisitorsの平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].min().rename(columns = {'visitors':'h_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'h_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].median().rename(columns = {'visitors':'h_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].max().rename(columns = {'visitors':'h_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].count().rename(columns = {'visitors':'h_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1) \n",
    "\n",
    "train['total_reserve_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserve_mean'] = (train['rv2_x'] + train['rv2_y'])/2\n",
    "train['total_reserve_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y'])/2\n",
    "\n",
    "test['total_reserve_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserve_mean'] = (test['rv2_x'] + test['rv2_y'])/2\n",
    "test['total_reserve_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y'])/2\n",
    "\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2']= lbl.fit_transform(test['air_store_id'])\n",
    "\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date', 'visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "\n",
    "all_data = pd.concat([train, test]) \n",
    "\n",
    "#指数移動平均の追加。これはなくても良いかも\n",
    "#https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46179#266344\n",
    "#def calc_shifted_ewm(series, alpha, adjust=True):\n",
    "#    return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n",
    "\n",
    "#train['ewm'] = train.groupby(['air_store_id', 'dow']).apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1)).sort_index(level=['air_store_id']).values\n",
    "\n",
    "#以下、気象データの追加\n",
    "df_air_store_weather_station = pd.read_csv('../../../mltestdata/05_recruit/air_store_info_with_nearest_active_station.csv')\n",
    "\n",
    "cols = ['air_store_id', 'station_id', 'station_latitude', 'station_longitude', 'station_vincenty', 'station_great_circle']\n",
    "all_data = pd.merge(all_data, df_air_store_weather_station[cols], on='air_store_id', how='left')\n",
    "\n",
    "combine = all_data\n",
    "filenames = []\n",
    "df_weather = None\n",
    "for station_id in combine['station_id'].unique():\n",
    "    fn = f\"../../../mltestdata/05_recruit/1-1-16_5-31-17_Weather/{station_id}.csv\"\n",
    "    if not fn in filenames:\n",
    "        df = pd.read_csv(fn)\n",
    "        df['station_id'] = station_id\n",
    "        if df_weather is None:\n",
    "            df_weather = df\n",
    "        else:\n",
    "            df_weather = pd.concat([df_weather, df])\n",
    "        del df\n",
    "\n",
    "        filenames.append(fn)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#欠損値を平均で穴埋め（median, ffillで試すも特に差は出なかった）\n",
    "df_weather = df_weather.fillna(df_weather.mean())\n",
    "\n",
    "df_weather = df_weather.rename(columns={'calendar_date': 'visit_date'})\n",
    "\n",
    "df_weather['visit_date'] = pd.to_datetime(df_weather['visit_date'])\n",
    "df_weather['visit_date'] = df_weather['visit_date'].dt.date\n",
    "\n",
    "#なんとなく対数化\n",
    "df_weather['precipitation'] = np.log1p(df_weather['precipitation'])\n",
    "\n",
    "#使いそうなデータだけ結合（その他の気象データは試していません。特に意味はなし）\n",
    "cols = ['station_id', \n",
    "    'visit_date', \n",
    "    'precipitation', \n",
    "    'hours_sunlight',\n",
    "    'avg_temperature',\n",
    "    'high_temperature',\n",
    "    'low_temperature']\n",
    "\n",
    "combine = pd.merge(combine, df_weather[cols], on=['station_id', 'visit_date'], how='left')\n",
    "\n",
    "#降水量をカテゴリ化\n",
    "def simplify_pre(df):\n",
    "    df.precipitation = df.precipitation.fillna(0)\n",
    "    bins = ( -1, 0.01, 2,  5)\n",
    "    group_names = ['1', '2', '3']\n",
    "    categories = pd.cut(df.precipitation, bins, labels=group_names)\n",
    "    df.precipitation = categories\n",
    "    return df\n",
    "\n",
    "combine = simplify_pre(combine) \n",
    "all_data = combine \n",
    "\n",
    "################################## modified on 1-Feb\n",
    "#不要そうなデータを削除\n",
    "#drop_col =['station_id', 'station_latitude','station_longitude','station_vincenty', 'station_great_circle','hours_sunlight','high_temperature','low_temperature']\n",
    "drop_col =['hours_sunlight','high_temperature','low_temperature']\n",
    "all_data = all_data.drop(drop_col, axis = 1)\n",
    "\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "\n",
    "################################## modified on 1-Feb\n",
    "#Encode station_id\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['station_id'] = lbl.fit_transform(train['station_id']) \n",
    "test['station_id'] = lbl.fit_transform(test['station_id']) \n",
    "\n",
    "\n",
    "\n",
    "#ID×降水量で平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].min().rename(columns = {'visitors':'p_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'p_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].median().rename(columns = {'visitors':'p_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation',])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].max().rename(columns = {'visitors':'p_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].count().rename(columns = {'visitors':'p_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "#countをLabel Encoder化。なんとなく試してみたら結果が良かった。\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['count_visitors'] = lbl.fit_transform(train['count_visitors']) \n",
    "test['count_visitors']= lbl.fit_transform(test['count_visitors'])\n",
    "train['m_count_visitors'] = lbl.fit_transform(train['m_count_visitors'])\n",
    "test['m_count_visitors']= lbl.fit_transform(test['m_count_visitors'])\n",
    "train['h_count_reserves'] = lbl.fit_transform(train['h_count_visitors'])\n",
    "test['h_count_reserves']= lbl.fit_transform(test['h_count_visitors'])\n",
    "train['p_count_visitors'] = lbl.fit_transform(train['p_count_visitors'])\n",
    "test['p_count_visitors']= lbl.fit_transform(test['p_count_visitors'])\n",
    "\n",
    "# GW flag\n",
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]\n",
    "\n",
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precipitation'] = train['precipitation'].values.astype(float)\n",
    "test['precipitation'] = test['precipitation'].values.astype(float)\n",
    "\n",
    "train_input['precipitation'] = train_input['precipitation'].values.astype(float)\n",
    "test_input['precipitation'] = test_input['precipitation'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: (252108, 81)\n",
      "316 for period 1: (47699, 81)\n",
      "316 for period 1 & 2: (126700, 81) <=== To be used\n",
      "513 for period 2 (125408, 81)\n",
      "316 for period 1 + 513 for period 2 (173107, 81) <=== To be used\n",
      "\n",
      "Total test: (32019, 81)\n",
      "test with 316(12207, 81)\n",
      "test with 513(19812, 81)\n"
     ]
    }
   ],
   "source": [
    "tra = train.copy()\n",
    "tes = test.copy()\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "#tra = pd.read_csv('/Users/suzukishinji/kaggle/recluit/air_visit_data.csv')\n",
    "#tes = pd.read_csv('/Users/suzukishinji/kaggle/recluit/sample_submission-3.csv')\n",
    "\n",
    "#countで試してみましたが、「2016年初めからデータはあるものの営業日の少ないお店」と「データは途中からだけど営業日の多い店」が混じるため、2016/1~2016/6に営業していた店と営業していないお店という分け方をしてみました。\n",
    "\n",
    "tra['visit_date'] = pd.to_datetime(tra['visit_date'])\n",
    "tra['visit_date'] = pd.to_datetime(tra['visit_date'])\n",
    "tra['dow'] = tra['visit_date'].dt.dayofweek\n",
    "tra['year'] = tra['visit_date'].dt.year\n",
    "tra['month'] = tra['visit_date'].dt.month\n",
    "tra['visit_date'] = tra['visit_date'].dt.date\n",
    "\n",
    "#2016/1~2016/6に営業しているお店は316店舗でした。\n",
    "year_2016 = tra[tra['year'] == 2016]\n",
    "month_1 = year_2016[year_2016['month'] == 1]\n",
    "month_2 = year_2016[year_2016['month'] == 2]\n",
    "month_3 = year_2016[year_2016['month'] == 3]\n",
    "month_4 = year_2016[year_2016['month'] == 4]\n",
    "month_5 = year_2016[year_2016['month'] == 5]\n",
    "month_6 = year_2016[year_2016['month'] == 6]\n",
    "tra_store_316_6 = pd.concat([month_1,month_2,month_3,month_4,month_5,month_6])\n",
    "\n",
    "id = list(tra_store_316_6['air_store_id'].values.flatten())\n",
    "\n",
    "#trainデータから2016/1~2016/6に営業しているお店の全期間を抜き出したもの。\n",
    "#trainデータから上記を除いたものが513店舗。\n",
    "tra_store_316_all = tra[tra['air_store_id'].isin(id)]\n",
    "tra_store_513 = tra[~tra['air_store_id'].isin(id)] # これはNOTなんだ。\n",
    "\n",
    "tes['visit_date'] = tes['id'].map(lambda x: str(x).split('_')[2])\n",
    "tes['air_store_id'] = tes['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "tes['visit_date'] = pd.to_datetime(tes['visit_date'])\n",
    "tes['dow'] = tes['visit_date'].dt.dayofweek\n",
    "tes['year'] = tes['visit_date'].dt.year\n",
    "tes['month'] = tes['visit_date'].dt.month\n",
    "tes['visit_date'] = tes['visit_date'].dt.date\n",
    "\n",
    "#testデータから2016/1~2016/6に営業しているお店の全期間を抜き出したもの。\n",
    "#testデータから上記を除いたものが513店舗。\n",
    "tes_store_316 = tes[tes['air_store_id'].isin(id)]\n",
    "tes_store_513 = tes[~tes['air_store_id'].isin(id)]\n",
    "\n",
    "#使うのは①2016/1~2016/6の期間のデータ、②2016/1~2016/6に営業していたお店の全期間のデータ、③途中からの513店舗のデータ。\n",
    "#tra_store_316_6[['air_store_id', 'visit_date','visitors']].to_csv('tra_store_316_6.csv', index = False)\n",
    "#tra_store_316_all[['air_store_id', 'visit_date','visitors']].to_csv('tra_store_316_all.csv', index = False)\n",
    "#tra_store_513[['air_store_id', 'visit_date','visitors']].to_csv('tra_store_513.csv', index = False)\n",
    "\n",
    "#使うのは①2016/1~2016/6の期間のデータ、②2016/1~2016/6に営業していたお店の全期間のデータ、③途中からの513店舗のデータ。\n",
    "tra_store_316_6.to_csv('tra_store_316_6.csv', index = False) #index部分がいらない場合はオプションを追加\n",
    "tra_store_316_all.to_csv('tra_store_316_all.csv', index = False)\n",
    "tra_store_513.to_csv('tra_store_513.csv', index = False)\n",
    "\n",
    "#テストデータは期間で分けずに最初からの316店舗と途中からの513店舗\n",
    "#tes_store_316[['id','visitors']].to_csv('tes_store_316.csv', index = False)\n",
    "#tes_store_513[['id','visitors']].to_csv('tes_store_513.csv', index = False)\n",
    "\n",
    "tes_store_316.to_csv('tes_store_316.csv', index = False)\n",
    "tes_store_513.to_csv('tes_store_513.csv', index = False)\n",
    "\n",
    "tra_316_6 = pd.read_csv('./tra_store_316_6.csv') # 316 for period 1 (first 6 months)\n",
    "tra_316_all = pd.read_csv('./tra_store_316_all.csv') # 316 for period 1 & 2\n",
    "tra_513 = pd.read_csv('./tra_store_513.csv') # 513 for period 2 (expect first 6 months)\n",
    "tes_316 = pd.read_csv('./tes_store_316.csv') # 316 only\n",
    "tes_513 = pd.read_csv('./tes_store_513.csv') # 513 only\n",
    "\n",
    "#出力されているか確認。（①と③はくっつけて使う）\n",
    "print(\"Total train: \"+str(tra.shape))\n",
    "print(\"316 for period 1: \"+str(tra_316_6.shape))\n",
    "print(\"316 for period 1 & 2: \"+str(tra_316_all.shape)+\" <=== To be used\")\n",
    "print(\"513 for period 2 \"+str(tra_513.shape))\n",
    "tra_513_316 = pd.concat([tra_316_6, tra_513], ignore_index=True)\n",
    "tra_513_316_index = pd.concat([tra_316_6, tra_513])\n",
    "print(\"316 for period 1 + 513 for period 2 \"+str(tra_513_316.shape)+\" <=== To be used\"+\"\\n\")\n",
    "print(\"Total test: \"+str(tes.shape))\n",
    "print(\"test with 316\"+str(tes_316.shape))\n",
    "print(\"test with 513\"+str(tes_513.shape))\n",
    "\n",
    "train_1 = tra_316_all.copy()\n",
    "test_1 = tes_316.copy()\n",
    "\n",
    "train_2 = tra_513_316.copy()\n",
    "test_2 = tes_513.copy()\n",
    "\n",
    "y_1 = train_1.visitors\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_1=train_1.drop(drop_cols, axis=1)\n",
    "test_1=test_1.drop(drop_cols, axis=1)\n",
    "\n",
    "y_2 = train_2.visitors\n",
    "\n",
    "train_2=train_2.drop(drop_cols, axis=1)\n",
    "test_2=test_2.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126700, 81)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_store_316_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125408, 81)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_store_513.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12207, 81)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes_store_316.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19812, 81)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes_store_513.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define utility function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, kf,  verbose=True, verbose_eval=50, scoreonly=False):\n",
    "    start_time=time.time()\n",
    "    nround=[]\n",
    "    # the prediction matrix need to contains 3 columns, one for the probability of each class\n",
    "    #train_pred = np.zeros((x_train.shape[0],3))\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    \n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "        #return 'rmsle', rmsle(true, preds), False\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "\n",
    "        #y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "        x_test_kf=x_test.copy()\n",
    "        \n",
    "        d_train = xgboost.DMatrix(x_train_kf, y_train_kf)\n",
    "        d_val=xgboost.DMatrix(x_val_kf, y_val_kf)\n",
    "        d_test = xgboost.DMatrix(x_test_kf)\n",
    "        \n",
    "        watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "        bst = xgboost.train(params=params, \n",
    "                            dtrain=d_train, \n",
    "                            num_boost_round=8000, \n",
    "                            early_stopping_rounds=100,\n",
    "                            evals=watchlist, \n",
    "                            verbose_eval=verbose_eval)        \n",
    "        \n",
    "#        y_val_kf_preds=bst.predict(d_val, ntree_limit=bst.best_ntree_limit)\n",
    "        y_val_kf_preds=np.expm1(bst.predict(d_val, ntree_limit=bst.best_ntree_limit))\n",
    "        nround.append(bst.best_ntree_limit)\n",
    "        \n",
    "        train_pred[val_index] += y_val_kf_preds\n",
    "#        test_pred += np.expm1((bst.predict(x_test, ntree_limit=bst.best_ntree_limit)))\n",
    "        test_pred += np.expm1(bst.predict(d_test))\n",
    "        \n",
    "        \n",
    "        #fold_cv = log_loss(y_val_kf.values, y_val_kf_preds)\n",
    "        #fold_rmsle = rmsle(np.expm1(train_pred[val_index]),np.expm1(y_val_kf.values))\n",
    "        fold_rmsle = rmsle(train_pred[val_index],np.expm1(y_val_kf.values))\n",
    "        fold_cv = fold_rmsle\n",
    "        \n",
    "        if verbose:\n",
    "            print('fold cv {} rmsle score is {:.6f}'.format(i, fold_cv))\n",
    "\n",
    "    test_pred = test_pred / kf.n_splits\n",
    "    #cv_score = log_loss(y_train, train_pred)\n",
    "    #cv_score = rmsle(np.expm1(train_pred), y_train)\n",
    "    cv_score = rmsle(train_pred, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv rmsle score is {:.6f}'.format(cv_score))    \n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    " \n",
    "    if scoreonly:\n",
    "        #return cv_score # for the purpose of bayesian optimisation, we only need to return the CV score\n",
    "        return cv_score\n",
    "    else:\n",
    "        return (cv_score,train_pred,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Modeling based on train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cross validation with xgboost__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-rmse:2.33018\tval-rmse:2.34643\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.531852\tval-rmse:0.543664\n",
      "[100]\ttrain-rmse:0.489632\tval-rmse:0.50142\n",
      "[150]\ttrain-rmse:0.483156\tval-rmse:0.495893\n",
      "[200]\ttrain-rmse:0.47874\tval-rmse:0.492569\n",
      "[250]\ttrain-rmse:0.475566\tval-rmse:0.490667\n",
      "[300]\ttrain-rmse:0.472716\tval-rmse:0.489322\n",
      "[350]\ttrain-rmse:0.470267\tval-rmse:0.487797\n",
      "[400]\ttrain-rmse:0.468569\tval-rmse:0.486893\n",
      "[450]\ttrain-rmse:0.467194\tval-rmse:0.486348\n",
      "[500]\ttrain-rmse:0.465902\tval-rmse:0.485863\n",
      "[550]\ttrain-rmse:0.464913\tval-rmse:0.485571\n",
      "[600]\ttrain-rmse:0.464201\tval-rmse:0.485316\n",
      "[650]\ttrain-rmse:0.463184\tval-rmse:0.484829\n",
      "[700]\ttrain-rmse:0.462553\tval-rmse:0.484527\n",
      "[750]\ttrain-rmse:0.461746\tval-rmse:0.484139\n",
      "[800]\ttrain-rmse:0.460964\tval-rmse:0.483947\n",
      "[850]\ttrain-rmse:0.460234\tval-rmse:0.483817\n",
      "[900]\ttrain-rmse:0.459619\tval-rmse:0.483697\n",
      "[950]\ttrain-rmse:0.459072\tval-rmse:0.483515\n",
      "[1000]\ttrain-rmse:0.458406\tval-rmse:0.483243\n",
      "[1050]\ttrain-rmse:0.457826\tval-rmse:0.483136\n",
      "[1100]\ttrain-rmse:0.45703\tval-rmse:0.483038\n",
      "[1150]\ttrain-rmse:0.456383\tval-rmse:0.482889\n",
      "[1200]\ttrain-rmse:0.455687\tval-rmse:0.482758\n",
      "[1250]\ttrain-rmse:0.455258\tval-rmse:0.48266\n",
      "[1300]\ttrain-rmse:0.454481\tval-rmse:0.482497\n",
      "[1350]\ttrain-rmse:0.45388\tval-rmse:0.482354\n",
      "[1400]\ttrain-rmse:0.453099\tval-rmse:0.482149\n",
      "[1450]\ttrain-rmse:0.452263\tval-rmse:0.481947\n",
      "[1500]\ttrain-rmse:0.451819\tval-rmse:0.481822\n",
      "[1550]\ttrain-rmse:0.451339\tval-rmse:0.481749\n",
      "[1600]\ttrain-rmse:0.450658\tval-rmse:0.481672\n",
      "[1650]\ttrain-rmse:0.450032\tval-rmse:0.481556\n",
      "[1700]\ttrain-rmse:0.449634\tval-rmse:0.481574\n",
      "[1750]\ttrain-rmse:0.449034\tval-rmse:0.481446\n",
      "[1800]\ttrain-rmse:0.44852\tval-rmse:0.481381\n",
      "[1850]\ttrain-rmse:0.44796\tval-rmse:0.481339\n",
      "[1900]\ttrain-rmse:0.447553\tval-rmse:0.481231\n",
      "[1950]\ttrain-rmse:0.447072\tval-rmse:0.48124\n",
      "[2000]\ttrain-rmse:0.44654\tval-rmse:0.481124\n",
      "[2050]\ttrain-rmse:0.445901\tval-rmse:0.480937\n",
      "[2100]\ttrain-rmse:0.445394\tval-rmse:0.480899\n",
      "[2150]\ttrain-rmse:0.444911\tval-rmse:0.480721\n",
      "[2200]\ttrain-rmse:0.444476\tval-rmse:0.480693\n",
      "[2250]\ttrain-rmse:0.443844\tval-rmse:0.480696\n",
      "Stopping. Best iteration:\n",
      "[2173]\ttrain-rmse:0.444738\tval-rmse:0.480663\n",
      "\n",
      "[0]\ttrain-rmse:2.33148\tval-rmse:2.33506\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.532957\tval-rmse:0.535296\n",
      "[100]\ttrain-rmse:0.490415\tval-rmse:0.494478\n",
      "[150]\ttrain-rmse:0.483149\tval-rmse:0.489154\n",
      "[200]\ttrain-rmse:0.478135\tval-rmse:0.485864\n",
      "[250]\ttrain-rmse:0.474412\tval-rmse:0.48377\n",
      "[300]\ttrain-rmse:0.471517\tval-rmse:0.482302\n",
      "[350]\ttrain-rmse:0.46924\tval-rmse:0.481361\n",
      "[400]\ttrain-rmse:0.467303\tval-rmse:0.480597\n",
      "[450]\ttrain-rmse:0.465677\tval-rmse:0.480098\n",
      "[500]\ttrain-rmse:0.464228\tval-rmse:0.479428\n",
      "[550]\ttrain-rmse:0.462918\tval-rmse:0.478788\n",
      "[600]\ttrain-rmse:0.461565\tval-rmse:0.478311\n",
      "[650]\ttrain-rmse:0.460636\tval-rmse:0.478082\n",
      "[700]\ttrain-rmse:0.459687\tval-rmse:0.477756\n",
      "[750]\ttrain-rmse:0.458776\tval-rmse:0.477336\n",
      "[800]\ttrain-rmse:0.457907\tval-rmse:0.47703\n",
      "[850]\ttrain-rmse:0.45699\tval-rmse:0.476833\n",
      "[900]\ttrain-rmse:0.456063\tval-rmse:0.476618\n",
      "[950]\ttrain-rmse:0.455308\tval-rmse:0.476425\n",
      "[1000]\ttrain-rmse:0.454676\tval-rmse:0.476224\n",
      "[1050]\ttrain-rmse:0.453959\tval-rmse:0.476151\n",
      "[1100]\ttrain-rmse:0.453083\tval-rmse:0.475721\n",
      "[1150]\ttrain-rmse:0.452435\tval-rmse:0.475623\n",
      "[1200]\ttrain-rmse:0.451588\tval-rmse:0.475558\n",
      "[1250]\ttrain-rmse:0.450813\tval-rmse:0.475397\n",
      "[1300]\ttrain-rmse:0.449862\tval-rmse:0.475281\n",
      "[1350]\ttrain-rmse:0.448939\tval-rmse:0.475163\n",
      "[1400]\ttrain-rmse:0.448033\tval-rmse:0.475121\n",
      "[1450]\ttrain-rmse:0.44711\tval-rmse:0.474986\n",
      "[1500]\ttrain-rmse:0.446261\tval-rmse:0.474849\n",
      "[1550]\ttrain-rmse:0.445257\tval-rmse:0.474671\n",
      "[1600]\ttrain-rmse:0.444403\tval-rmse:0.474521\n",
      "[1650]\ttrain-rmse:0.443541\tval-rmse:0.474407\n",
      "[1700]\ttrain-rmse:0.442721\tval-rmse:0.474355\n",
      "[1750]\ttrain-rmse:0.442029\tval-rmse:0.474287\n",
      "[1800]\ttrain-rmse:0.4415\tval-rmse:0.474333\n",
      "[1850]\ttrain-rmse:0.440769\tval-rmse:0.474184\n",
      "[1900]\ttrain-rmse:0.440209\tval-rmse:0.474049\n",
      "[1950]\ttrain-rmse:0.439499\tval-rmse:0.47394\n",
      "[2000]\ttrain-rmse:0.439037\tval-rmse:0.47395\n",
      "[2050]\ttrain-rmse:0.438521\tval-rmse:0.473922\n",
      "[2100]\ttrain-rmse:0.437848\tval-rmse:0.473853\n",
      "[2150]\ttrain-rmse:0.437268\tval-rmse:0.473866\n",
      "[2200]\ttrain-rmse:0.436558\tval-rmse:0.473778\n",
      "[2250]\ttrain-rmse:0.436011\tval-rmse:0.473821\n",
      "[2300]\ttrain-rmse:0.435468\tval-rmse:0.473732\n",
      "[2350]\ttrain-rmse:0.43493\tval-rmse:0.473809\n",
      "Stopping. Best iteration:\n",
      "[2288]\ttrain-rmse:0.43556\tval-rmse:0.473712\n",
      "\n",
      "[0]\ttrain-rmse:2.33189\tval-rmse:2.331\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.531941\tval-rmse:0.532989\n",
      "[100]\ttrain-rmse:0.489589\tval-rmse:0.495562\n",
      "[150]\ttrain-rmse:0.482924\tval-rmse:0.490459\n",
      "[200]\ttrain-rmse:0.478578\tval-rmse:0.487484\n",
      "[250]\ttrain-rmse:0.475523\tval-rmse:0.485529\n",
      "[300]\ttrain-rmse:0.472944\tval-rmse:0.483916\n",
      "[350]\ttrain-rmse:0.470998\tval-rmse:0.482987\n",
      "[400]\ttrain-rmse:0.469029\tval-rmse:0.48209\n",
      "[450]\ttrain-rmse:0.466878\tval-rmse:0.480964\n",
      "[500]\ttrain-rmse:0.465675\tval-rmse:0.480489\n",
      "[550]\ttrain-rmse:0.464814\tval-rmse:0.480325\n",
      "[600]\ttrain-rmse:0.464268\tval-rmse:0.480235\n",
      "[650]\ttrain-rmse:0.46376\tval-rmse:0.480063\n",
      "[700]\ttrain-rmse:0.462787\tval-rmse:0.479767\n",
      "[750]\ttrain-rmse:0.461878\tval-rmse:0.479676\n",
      "[800]\ttrain-rmse:0.460855\tval-rmse:0.479321\n",
      "[850]\ttrain-rmse:0.460216\tval-rmse:0.479273\n",
      "[900]\ttrain-rmse:0.459291\tval-rmse:0.478826\n",
      "[950]\ttrain-rmse:0.458304\tval-rmse:0.47852\n",
      "[1000]\ttrain-rmse:0.45734\tval-rmse:0.478235\n",
      "[1050]\ttrain-rmse:0.456194\tval-rmse:0.477895\n",
      "[1100]\ttrain-rmse:0.455665\tval-rmse:0.477677\n",
      "[1150]\ttrain-rmse:0.454911\tval-rmse:0.477593\n",
      "[1200]\ttrain-rmse:0.454002\tval-rmse:0.477438\n",
      "[1250]\ttrain-rmse:0.453134\tval-rmse:0.47713\n",
      "[1300]\ttrain-rmse:0.452084\tval-rmse:0.476816\n",
      "[1350]\ttrain-rmse:0.4513\tval-rmse:0.476621\n",
      "[1400]\ttrain-rmse:0.450242\tval-rmse:0.476373\n",
      "[1450]\ttrain-rmse:0.449288\tval-rmse:0.476194\n",
      "[1500]\ttrain-rmse:0.448622\tval-rmse:0.475998\n",
      "[1550]\ttrain-rmse:0.447727\tval-rmse:0.475848\n",
      "[1600]\ttrain-rmse:0.447008\tval-rmse:0.475857\n",
      "[1650]\ttrain-rmse:0.446293\tval-rmse:0.475636\n",
      "[1700]\ttrain-rmse:0.44542\tval-rmse:0.475501\n",
      "[1750]\ttrain-rmse:0.444682\tval-rmse:0.475352\n",
      "[1800]\ttrain-rmse:0.443982\tval-rmse:0.475238\n",
      "[1850]\ttrain-rmse:0.443332\tval-rmse:0.475256\n",
      "[1900]\ttrain-rmse:0.442593\tval-rmse:0.475123\n",
      "[1950]\ttrain-rmse:0.441855\tval-rmse:0.47501\n",
      "[2000]\ttrain-rmse:0.441132\tval-rmse:0.474959\n",
      "[2050]\ttrain-rmse:0.440279\tval-rmse:0.474875\n",
      "[2100]\ttrain-rmse:0.439487\tval-rmse:0.474794\n",
      "[2150]\ttrain-rmse:0.43885\tval-rmse:0.474802\n",
      "[2200]\ttrain-rmse:0.438282\tval-rmse:0.474722\n",
      "[2250]\ttrain-rmse:0.4375\tval-rmse:0.474684\n",
      "[2300]\ttrain-rmse:0.436696\tval-rmse:0.474546\n",
      "[2350]\ttrain-rmse:0.436006\tval-rmse:0.474575\n",
      "[2400]\ttrain-rmse:0.435446\tval-rmse:0.474548\n",
      "[2450]\ttrain-rmse:0.434646\tval-rmse:0.474468\n",
      "[2500]\ttrain-rmse:0.434021\tval-rmse:0.474498\n",
      "[2550]\ttrain-rmse:0.433279\tval-rmse:0.474475\n",
      "[2600]\ttrain-rmse:0.432593\tval-rmse:0.474511\n",
      "[2650]\ttrain-rmse:0.431954\tval-rmse:0.47455\n",
      "Stopping. Best iteration:\n",
      "[2559]\ttrain-rmse:0.433132\tval-rmse:0.474449\n",
      "\n",
      "[0]\ttrain-rmse:2.33205\tval-rmse:2.33014\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.532644\tval-rmse:0.534636\n",
      "[100]\ttrain-rmse:0.489877\tval-rmse:0.493441\n",
      "[150]\ttrain-rmse:0.483084\tval-rmse:0.488428\n",
      "[200]\ttrain-rmse:0.47942\tval-rmse:0.486495\n",
      "[250]\ttrain-rmse:0.476997\tval-rmse:0.485292\n",
      "[300]\ttrain-rmse:0.475002\tval-rmse:0.484422\n",
      "[350]\ttrain-rmse:0.473367\tval-rmse:0.48378\n",
      "[400]\ttrain-rmse:0.471474\tval-rmse:0.482869\n",
      "[450]\ttrain-rmse:0.46997\tval-rmse:0.482394\n",
      "[500]\ttrain-rmse:0.468286\tval-rmse:0.481889\n",
      "[550]\ttrain-rmse:0.467007\tval-rmse:0.481582\n",
      "[600]\ttrain-rmse:0.465623\tval-rmse:0.481249\n",
      "[650]\ttrain-rmse:0.464622\tval-rmse:0.481024\n",
      "[700]\ttrain-rmse:0.463229\tval-rmse:0.480562\n",
      "[750]\ttrain-rmse:0.462169\tval-rmse:0.480397\n",
      "[800]\ttrain-rmse:0.461076\tval-rmse:0.479971\n",
      "[850]\ttrain-rmse:0.460243\tval-rmse:0.479626\n",
      "[900]\ttrain-rmse:0.459516\tval-rmse:0.479582\n",
      "[950]\ttrain-rmse:0.458548\tval-rmse:0.479305\n",
      "[1000]\ttrain-rmse:0.457947\tval-rmse:0.479234\n",
      "[1050]\ttrain-rmse:0.457002\tval-rmse:0.478993\n",
      "[1100]\ttrain-rmse:0.456044\tval-rmse:0.478772\n",
      "[1150]\ttrain-rmse:0.455193\tval-rmse:0.478564\n",
      "[1200]\ttrain-rmse:0.454052\tval-rmse:0.478389\n",
      "[1250]\ttrain-rmse:0.453246\tval-rmse:0.478318\n",
      "[1300]\ttrain-rmse:0.452628\tval-rmse:0.478185\n",
      "[1350]\ttrain-rmse:0.451834\tval-rmse:0.478054\n",
      "[1400]\ttrain-rmse:0.450908\tval-rmse:0.478004\n",
      "[1450]\ttrain-rmse:0.450353\tval-rmse:0.478077\n",
      "[1500]\ttrain-rmse:0.449474\tval-rmse:0.477917\n",
      "[1550]\ttrain-rmse:0.448869\tval-rmse:0.477981\n",
      "Stopping. Best iteration:\n",
      "[1492]\ttrain-rmse:0.44961\tval-rmse:0.477889\n",
      "\n",
      "[0]\ttrain-rmse:2.33227\tval-rmse:2.32865\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.531687\tval-rmse:0.534723\n",
      "[100]\ttrain-rmse:0.489313\tval-rmse:0.495146\n",
      "[150]\ttrain-rmse:0.482578\tval-rmse:0.490374\n",
      "[200]\ttrain-rmse:0.478634\tval-rmse:0.487956\n",
      "[250]\ttrain-rmse:0.476007\tval-rmse:0.486521\n",
      "[300]\ttrain-rmse:0.473955\tval-rmse:0.48557\n",
      "[350]\ttrain-rmse:0.472229\tval-rmse:0.484708\n",
      "[400]\ttrain-rmse:0.470524\tval-rmse:0.483908\n",
      "[450]\ttrain-rmse:0.469156\tval-rmse:0.483528\n",
      "[500]\ttrain-rmse:0.46769\tval-rmse:0.483026\n",
      "[550]\ttrain-rmse:0.46646\tval-rmse:0.482543\n",
      "[600]\ttrain-rmse:0.465304\tval-rmse:0.482108\n",
      "[650]\ttrain-rmse:0.464409\tval-rmse:0.481921\n",
      "[700]\ttrain-rmse:0.463395\tval-rmse:0.48139\n",
      "[750]\ttrain-rmse:0.462382\tval-rmse:0.481026\n",
      "[800]\ttrain-rmse:0.461594\tval-rmse:0.480822\n",
      "[850]\ttrain-rmse:0.460582\tval-rmse:0.4805\n",
      "[900]\ttrain-rmse:0.45966\tval-rmse:0.480324\n",
      "[950]\ttrain-rmse:0.458928\tval-rmse:0.48025\n",
      "[1000]\ttrain-rmse:0.458187\tval-rmse:0.480078\n",
      "[1050]\ttrain-rmse:0.457214\tval-rmse:0.479781\n",
      "[1100]\ttrain-rmse:0.456401\tval-rmse:0.479595\n",
      "[1150]\ttrain-rmse:0.455871\tval-rmse:0.479555\n",
      "[1200]\ttrain-rmse:0.455164\tval-rmse:0.479395\n",
      "[1250]\ttrain-rmse:0.454642\tval-rmse:0.4794\n",
      "[1300]\ttrain-rmse:0.453852\tval-rmse:0.479203\n",
      "[1350]\ttrain-rmse:0.453239\tval-rmse:0.479182\n",
      "[1400]\ttrain-rmse:0.452546\tval-rmse:0.479066\n",
      "[1450]\ttrain-rmse:0.452038\tval-rmse:0.479012\n",
      "[1500]\ttrain-rmse:0.451562\tval-rmse:0.478883\n",
      "[1550]\ttrain-rmse:0.450576\tval-rmse:0.478659\n",
      "[1600]\ttrain-rmse:0.449885\tval-rmse:0.478616\n",
      "[1650]\ttrain-rmse:0.449101\tval-rmse:0.47853\n",
      "[1700]\ttrain-rmse:0.448478\tval-rmse:0.478367\n",
      "[1750]\ttrain-rmse:0.447913\tval-rmse:0.47823\n",
      "[1800]\ttrain-rmse:0.447365\tval-rmse:0.478241\n",
      "[1850]\ttrain-rmse:0.446798\tval-rmse:0.478227\n",
      "Stopping. Best iteration:\n",
      "[1788]\ttrain-rmse:0.447509\tval-rmse:0.478193\n",
      "\n",
      "[0]\ttrain-rmse:2.33202\tval-rmse:2.33006\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.532597\tval-rmse:0.533785\n",
      "[100]\ttrain-rmse:0.490107\tval-rmse:0.494602\n",
      "[150]\ttrain-rmse:0.48333\tval-rmse:0.490291\n",
      "[200]\ttrain-rmse:0.478893\tval-rmse:0.487885\n",
      "[250]\ttrain-rmse:0.475764\tval-rmse:0.486208\n",
      "[300]\ttrain-rmse:0.473213\tval-rmse:0.485089\n",
      "[350]\ttrain-rmse:0.47172\tval-rmse:0.484553\n",
      "[400]\ttrain-rmse:0.470157\tval-rmse:0.483986\n",
      "[450]\ttrain-rmse:0.468171\tval-rmse:0.483259\n",
      "[500]\ttrain-rmse:0.466649\tval-rmse:0.48273\n",
      "[550]\ttrain-rmse:0.465351\tval-rmse:0.482251\n",
      "[600]\ttrain-rmse:0.464306\tval-rmse:0.481934\n",
      "[650]\ttrain-rmse:0.463388\tval-rmse:0.481596\n",
      "[700]\ttrain-rmse:0.462437\tval-rmse:0.481327\n",
      "[750]\ttrain-rmse:0.461181\tval-rmse:0.480955\n",
      "[800]\ttrain-rmse:0.460307\tval-rmse:0.480779\n",
      "[850]\ttrain-rmse:0.459659\tval-rmse:0.480704\n",
      "[900]\ttrain-rmse:0.458617\tval-rmse:0.480526\n",
      "[950]\ttrain-rmse:0.457959\tval-rmse:0.480362\n",
      "[1000]\ttrain-rmse:0.457255\tval-rmse:0.480263\n",
      "[1050]\ttrain-rmse:0.45669\tval-rmse:0.480282\n",
      "[1100]\ttrain-rmse:0.455843\tval-rmse:0.480066\n",
      "[1150]\ttrain-rmse:0.455163\tval-rmse:0.479959\n",
      "[1200]\ttrain-rmse:0.45444\tval-rmse:0.479804\n",
      "[1250]\ttrain-rmse:0.453896\tval-rmse:0.479826\n",
      "[1300]\ttrain-rmse:0.453337\tval-rmse:0.479772\n",
      "[1350]\ttrain-rmse:0.452701\tval-rmse:0.479563\n",
      "[1400]\ttrain-rmse:0.452169\tval-rmse:0.479417\n",
      "[1450]\ttrain-rmse:0.451694\tval-rmse:0.479308\n",
      "[1500]\ttrain-rmse:0.451091\tval-rmse:0.479148\n",
      "[1550]\ttrain-rmse:0.450403\tval-rmse:0.479007\n",
      "[1600]\ttrain-rmse:0.449893\tval-rmse:0.478921\n",
      "[1650]\ttrain-rmse:0.449184\tval-rmse:0.478845\n",
      "[1700]\ttrain-rmse:0.448487\tval-rmse:0.478738\n",
      "[1750]\ttrain-rmse:0.447825\tval-rmse:0.478577\n",
      "[1800]\ttrain-rmse:0.447118\tval-rmse:0.478357\n",
      "[1850]\ttrain-rmse:0.446665\tval-rmse:0.478252\n",
      "[1900]\ttrain-rmse:0.445999\tval-rmse:0.478098\n",
      "[1950]\ttrain-rmse:0.445148\tval-rmse:0.478047\n",
      "[2000]\ttrain-rmse:0.444635\tval-rmse:0.477976\n",
      "[2050]\ttrain-rmse:0.444138\tval-rmse:0.477908\n",
      "[2100]\ttrain-rmse:0.443274\tval-rmse:0.477702\n",
      "[2150]\ttrain-rmse:0.442479\tval-rmse:0.47756\n",
      "[2200]\ttrain-rmse:0.442073\tval-rmse:0.477499\n",
      "[2250]\ttrain-rmse:0.441275\tval-rmse:0.477341\n",
      "[2300]\ttrain-rmse:0.44041\tval-rmse:0.477188\n",
      "[2350]\ttrain-rmse:0.439694\tval-rmse:0.477205\n",
      "[2400]\ttrain-rmse:0.439139\tval-rmse:0.477262\n",
      "Stopping. Best iteration:\n",
      "[2321]\ttrain-rmse:0.440061\tval-rmse:0.477154\n",
      "\n",
      "[0]\ttrain-rmse:2.33213\tval-rmse:2.32869\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.532576\tval-rmse:0.536021\n",
      "[100]\ttrain-rmse:0.490148\tval-rmse:0.496414\n",
      "[150]\ttrain-rmse:0.483018\tval-rmse:0.491648\n",
      "[200]\ttrain-rmse:0.47829\tval-rmse:0.488912\n",
      "[250]\ttrain-rmse:0.474674\tval-rmse:0.487175\n",
      "[300]\ttrain-rmse:0.471836\tval-rmse:0.485838\n",
      "[350]\ttrain-rmse:0.469512\tval-rmse:0.484896\n",
      "[400]\ttrain-rmse:0.467396\tval-rmse:0.484226\n",
      "[450]\ttrain-rmse:0.465606\tval-rmse:0.483658\n",
      "[500]\ttrain-rmse:0.464151\tval-rmse:0.483059\n",
      "[550]\ttrain-rmse:0.462976\tval-rmse:0.482817\n",
      "[600]\ttrain-rmse:0.461595\tval-rmse:0.482506\n",
      "[650]\ttrain-rmse:0.460518\tval-rmse:0.482346\n",
      "[700]\ttrain-rmse:0.459381\tval-rmse:0.481985\n",
      "[750]\ttrain-rmse:0.458507\tval-rmse:0.481838\n",
      "[800]\ttrain-rmse:0.457769\tval-rmse:0.481652\n",
      "[850]\ttrain-rmse:0.456897\tval-rmse:0.481523\n",
      "[900]\ttrain-rmse:0.456224\tval-rmse:0.481348\n",
      "[950]\ttrain-rmse:0.455258\tval-rmse:0.48117\n",
      "[1000]\ttrain-rmse:0.454372\tval-rmse:0.480972\n",
      "[1050]\ttrain-rmse:0.453709\tval-rmse:0.480775\n",
      "[1100]\ttrain-rmse:0.453284\tval-rmse:0.480774\n",
      "[1150]\ttrain-rmse:0.452539\tval-rmse:0.480658\n",
      "[1200]\ttrain-rmse:0.451778\tval-rmse:0.480514\n",
      "[1250]\ttrain-rmse:0.451171\tval-rmse:0.480385\n",
      "[1300]\ttrain-rmse:0.450559\tval-rmse:0.480277\n",
      "[1350]\ttrain-rmse:0.449886\tval-rmse:0.48018\n",
      "[1400]\ttrain-rmse:0.448915\tval-rmse:0.479989\n",
      "[1450]\ttrain-rmse:0.448214\tval-rmse:0.479912\n",
      "[1500]\ttrain-rmse:0.447285\tval-rmse:0.47983\n",
      "[1550]\ttrain-rmse:0.446468\tval-rmse:0.479717\n",
      "[1600]\ttrain-rmse:0.445346\tval-rmse:0.479597\n",
      "[1650]\ttrain-rmse:0.444548\tval-rmse:0.479569\n",
      "[1700]\ttrain-rmse:0.443582\tval-rmse:0.479412\n",
      "[1750]\ttrain-rmse:0.442759\tval-rmse:0.479262\n",
      "[1800]\ttrain-rmse:0.441819\tval-rmse:0.479212\n",
      "[1850]\ttrain-rmse:0.441206\tval-rmse:0.479125\n",
      "[1900]\ttrain-rmse:0.440388\tval-rmse:0.479038\n",
      "[1950]\ttrain-rmse:0.439666\tval-rmse:0.47907\n",
      "[2000]\ttrain-rmse:0.438859\tval-rmse:0.478894\n",
      "[2050]\ttrain-rmse:0.437987\tval-rmse:0.478828\n",
      "[2100]\ttrain-rmse:0.437046\tval-rmse:0.478706\n",
      "[2150]\ttrain-rmse:0.436197\tval-rmse:0.478717\n",
      "[2200]\ttrain-rmse:0.435533\tval-rmse:0.478694\n",
      "[2250]\ttrain-rmse:0.434756\tval-rmse:0.478628\n",
      "[2300]\ttrain-rmse:0.434022\tval-rmse:0.478575\n",
      "[2350]\ttrain-rmse:0.433325\tval-rmse:0.478516\n",
      "[2400]\ttrain-rmse:0.432711\tval-rmse:0.478437\n",
      "[2450]\ttrain-rmse:0.432031\tval-rmse:0.478361\n",
      "[2500]\ttrain-rmse:0.431197\tval-rmse:0.478348\n",
      "Stopping. Best iteration:\n",
      "[2428]\ttrain-rmse:0.43232\tval-rmse:0.478334\n",
      "\n",
      "[0]\ttrain-rmse:2.33251\tval-rmse:2.33329\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.531821\tval-rmse:0.535621\n",
      "[100]\ttrain-rmse:0.489524\tval-rmse:0.495305\n",
      "[150]\ttrain-rmse:0.482825\tval-rmse:0.490844\n",
      "[200]\ttrain-rmse:0.478686\tval-rmse:0.488557\n",
      "[250]\ttrain-rmse:0.475155\tval-rmse:0.486391\n",
      "[300]\ttrain-rmse:0.472333\tval-rmse:0.485065\n",
      "[350]\ttrain-rmse:0.470072\tval-rmse:0.484132\n",
      "[400]\ttrain-rmse:0.467958\tval-rmse:0.48335\n",
      "[450]\ttrain-rmse:0.466644\tval-rmse:0.482934\n",
      "[500]\ttrain-rmse:0.465227\tval-rmse:0.482456\n",
      "[550]\ttrain-rmse:0.464057\tval-rmse:0.482144\n",
      "[600]\ttrain-rmse:0.462804\tval-rmse:0.481767\n",
      "[650]\ttrain-rmse:0.461539\tval-rmse:0.481277\n",
      "[700]\ttrain-rmse:0.460627\tval-rmse:0.481071\n",
      "[750]\ttrain-rmse:0.45985\tval-rmse:0.480861\n",
      "[800]\ttrain-rmse:0.458846\tval-rmse:0.480553\n",
      "[850]\ttrain-rmse:0.457907\tval-rmse:0.480419\n",
      "[900]\ttrain-rmse:0.457108\tval-rmse:0.480237\n",
      "[950]\ttrain-rmse:0.456234\tval-rmse:0.480144\n",
      "[1000]\ttrain-rmse:0.455635\tval-rmse:0.479996\n",
      "[1050]\ttrain-rmse:0.454912\tval-rmse:0.479964\n",
      "[1100]\ttrain-rmse:0.454157\tval-rmse:0.479935\n",
      "[1150]\ttrain-rmse:0.453398\tval-rmse:0.479774\n",
      "[1200]\ttrain-rmse:0.452343\tval-rmse:0.479542\n",
      "[1250]\ttrain-rmse:0.451602\tval-rmse:0.479329\n",
      "[1300]\ttrain-rmse:0.450939\tval-rmse:0.479246\n",
      "[1350]\ttrain-rmse:0.450072\tval-rmse:0.479067\n",
      "[1400]\ttrain-rmse:0.449192\tval-rmse:0.478956\n",
      "[1450]\ttrain-rmse:0.448577\tval-rmse:0.478712\n",
      "[1500]\ttrain-rmse:0.448004\tval-rmse:0.478613\n",
      "[1550]\ttrain-rmse:0.447157\tval-rmse:0.478507\n",
      "[1600]\ttrain-rmse:0.446409\tval-rmse:0.478409\n",
      "[1650]\ttrain-rmse:0.445779\tval-rmse:0.478436\n",
      "[1700]\ttrain-rmse:0.445084\tval-rmse:0.478398\n",
      "[1750]\ttrain-rmse:0.444462\tval-rmse:0.47831\n",
      "[1800]\ttrain-rmse:0.443669\tval-rmse:0.478216\n",
      "[1850]\ttrain-rmse:0.442981\tval-rmse:0.478193\n",
      "[1900]\ttrain-rmse:0.442279\tval-rmse:0.478118\n",
      "[1950]\ttrain-rmse:0.441506\tval-rmse:0.478021\n",
      "[2000]\ttrain-rmse:0.440993\tval-rmse:0.478057\n",
      "[2050]\ttrain-rmse:0.440263\tval-rmse:0.477891\n",
      "[2100]\ttrain-rmse:0.43956\tval-rmse:0.477812\n",
      "[2150]\ttrain-rmse:0.438936\tval-rmse:0.477828\n",
      "Stopping. Best iteration:\n",
      "[2094]\ttrain-rmse:0.439666\tval-rmse:0.477806\n",
      "\n",
      "[0]\ttrain-rmse:2.33206\tval-rmse:2.33018\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.532388\tval-rmse:0.532638\n",
      "[100]\ttrain-rmse:0.489886\tval-rmse:0.491237\n",
      "[150]\ttrain-rmse:0.483163\tval-rmse:0.486391\n",
      "[200]\ttrain-rmse:0.478671\tval-rmse:0.483529\n",
      "[250]\ttrain-rmse:0.475479\tval-rmse:0.481568\n",
      "[300]\ttrain-rmse:0.472442\tval-rmse:0.480178\n",
      "[350]\ttrain-rmse:0.470239\tval-rmse:0.479222\n",
      "[400]\ttrain-rmse:0.468501\tval-rmse:0.478446\n",
      "[450]\ttrain-rmse:0.467301\tval-rmse:0.477983\n",
      "[500]\ttrain-rmse:0.466033\tval-rmse:0.477537\n",
      "[550]\ttrain-rmse:0.465113\tval-rmse:0.477261\n",
      "[600]\ttrain-rmse:0.464283\tval-rmse:0.476972\n",
      "[650]\ttrain-rmse:0.463411\tval-rmse:0.476645\n",
      "[700]\ttrain-rmse:0.462322\tval-rmse:0.476297\n",
      "[750]\ttrain-rmse:0.461596\tval-rmse:0.476163\n",
      "[800]\ttrain-rmse:0.460899\tval-rmse:0.476048\n",
      "[850]\ttrain-rmse:0.459848\tval-rmse:0.475627\n",
      "[900]\ttrain-rmse:0.458839\tval-rmse:0.475507\n",
      "[950]\ttrain-rmse:0.457937\tval-rmse:0.475236\n",
      "[1000]\ttrain-rmse:0.457113\tval-rmse:0.475019\n",
      "[1050]\ttrain-rmse:0.456585\tval-rmse:0.474929\n",
      "[1100]\ttrain-rmse:0.455557\tval-rmse:0.474636\n",
      "[1150]\ttrain-rmse:0.45476\tval-rmse:0.474529\n",
      "[1200]\ttrain-rmse:0.454034\tval-rmse:0.474238\n",
      "[1250]\ttrain-rmse:0.453166\tval-rmse:0.474028\n",
      "[1300]\ttrain-rmse:0.452593\tval-rmse:0.47383\n",
      "[1350]\ttrain-rmse:0.45191\tval-rmse:0.473577\n",
      "[1400]\ttrain-rmse:0.451218\tval-rmse:0.47341\n",
      "[1450]\ttrain-rmse:0.450467\tval-rmse:0.473238\n",
      "[1500]\ttrain-rmse:0.449837\tval-rmse:0.473165\n",
      "[1550]\ttrain-rmse:0.449126\tval-rmse:0.473053\n",
      "[1600]\ttrain-rmse:0.448253\tval-rmse:0.472851\n",
      "[1650]\ttrain-rmse:0.447565\tval-rmse:0.472772\n",
      "[1700]\ttrain-rmse:0.446834\tval-rmse:0.472585\n",
      "[1750]\ttrain-rmse:0.446291\tval-rmse:0.472464\n",
      "[1800]\ttrain-rmse:0.445748\tval-rmse:0.472431\n",
      "[1850]\ttrain-rmse:0.445106\tval-rmse:0.472231\n",
      "[1900]\ttrain-rmse:0.444382\tval-rmse:0.472177\n",
      "[1950]\ttrain-rmse:0.443716\tval-rmse:0.472168\n",
      "Stopping. Best iteration:\n",
      "[1887]\ttrain-rmse:0.444545\tval-rmse:0.472121\n",
      "\n",
      "[0]\ttrain-rmse:2.33258\tval-rmse:2.32586\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.532434\tval-rmse:0.531231\n",
      "[100]\ttrain-rmse:0.489839\tval-rmse:0.492297\n",
      "[150]\ttrain-rmse:0.482722\tval-rmse:0.487295\n",
      "[200]\ttrain-rmse:0.47815\tval-rmse:0.484628\n",
      "[250]\ttrain-rmse:0.47497\tval-rmse:0.483171\n",
      "[300]\ttrain-rmse:0.472565\tval-rmse:0.482037\n",
      "[350]\ttrain-rmse:0.470274\tval-rmse:0.480949\n",
      "[400]\ttrain-rmse:0.468344\tval-rmse:0.480221\n",
      "[450]\ttrain-rmse:0.46649\tval-rmse:0.479527\n",
      "[500]\ttrain-rmse:0.465104\tval-rmse:0.479041\n",
      "[550]\ttrain-rmse:0.463863\tval-rmse:0.478356\n",
      "[600]\ttrain-rmse:0.462878\tval-rmse:0.477911\n",
      "[650]\ttrain-rmse:0.461581\tval-rmse:0.477624\n",
      "[700]\ttrain-rmse:0.460696\tval-rmse:0.477368\n",
      "[750]\ttrain-rmse:0.45981\tval-rmse:0.477141\n",
      "[800]\ttrain-rmse:0.458688\tval-rmse:0.47677\n",
      "[850]\ttrain-rmse:0.457927\tval-rmse:0.476557\n",
      "[900]\ttrain-rmse:0.457119\tval-rmse:0.476374\n",
      "[950]\ttrain-rmse:0.456362\tval-rmse:0.476113\n",
      "[1000]\ttrain-rmse:0.455469\tval-rmse:0.475943\n",
      "[1050]\ttrain-rmse:0.454719\tval-rmse:0.47594\n",
      "[1100]\ttrain-rmse:0.45385\tval-rmse:0.475824\n",
      "[1150]\ttrain-rmse:0.453074\tval-rmse:0.47567\n",
      "[1200]\ttrain-rmse:0.452258\tval-rmse:0.475505\n",
      "[1250]\ttrain-rmse:0.451455\tval-rmse:0.475336\n",
      "[1300]\ttrain-rmse:0.45049\tval-rmse:0.475081\n",
      "[1350]\ttrain-rmse:0.449833\tval-rmse:0.475044\n",
      "[1400]\ttrain-rmse:0.449037\tval-rmse:0.47496\n",
      "[1450]\ttrain-rmse:0.448281\tval-rmse:0.474851\n",
      "[1500]\ttrain-rmse:0.447326\tval-rmse:0.474665\n",
      "[1550]\ttrain-rmse:0.446669\tval-rmse:0.474531\n",
      "[1600]\ttrain-rmse:0.445728\tval-rmse:0.474408\n",
      "[1650]\ttrain-rmse:0.444804\tval-rmse:0.474238\n",
      "[1700]\ttrain-rmse:0.443898\tval-rmse:0.473992\n",
      "[1750]\ttrain-rmse:0.443029\tval-rmse:0.473844\n",
      "[1800]\ttrain-rmse:0.442075\tval-rmse:0.473627\n",
      "[1850]\ttrain-rmse:0.441048\tval-rmse:0.473554\n",
      "[1900]\ttrain-rmse:0.440183\tval-rmse:0.473449\n",
      "[1950]\ttrain-rmse:0.439193\tval-rmse:0.473418\n",
      "[2000]\ttrain-rmse:0.438293\tval-rmse:0.473431\n",
      "[2050]\ttrain-rmse:0.437407\tval-rmse:0.473315\n",
      "[2100]\ttrain-rmse:0.43646\tval-rmse:0.473231\n",
      "[2150]\ttrain-rmse:0.43568\tval-rmse:0.473292\n",
      "Stopping. Best iteration:\n",
      "[2074]\ttrain-rmse:0.436945\tval-rmse:0.473179\n",
      "\n",
      "cv score is 0.476361\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    #\"num_class\" : 3,\n",
    "    #\"tree_method\" : \"hist\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.05,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "cv_score =cross_validate_xgb(xgb_params, train_1, y_1, test_1, kf, verbose=False, verbose_eval=50, scoreonly=True)\n",
    "\n",
    "print('cv rmsle score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayesian Optimsation - Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth':(4,10),\n",
    "        'learning_rate':(0.05,0.3),\n",
    "        'subsample': (0.4, 1),\n",
    "        'colsample_bytree': (0.4, 1),\n",
    "        'gamma': (0.001, 10.0),\n",
    "        'min_child_weight': (0, 20),\n",
    "        'max_delta_step': (0, 10),\n",
    "        'n_estimators': (10, 25),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'max_features': (0.1, 0.999)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(xgb_wrapper)\n",
    "def xgbcv_func(max_depth, learning_rate, subsample, \n",
    "               colsample_bytree, gamma, min_child_weight, \n",
    "               max_delta_step, n_estimators, \n",
    "               min_samples_split, max_features,nthread=4, seed=0):\n",
    "    params = {\n",
    "        \"objective\" : \"reg:linear\",\n",
    "        #\"num_class\" : 3,\n",
    "        #\"tree_method\" : \"hist\",\n",
    "        \"eval_metric\" : \"rmse\",\n",
    "        \"nthread\": nthread,\n",
    "        \"seed\" : 0,\n",
    "        'silent': 1,\n",
    "\n",
    "        \"eta\":learning_rate,  # default 0.3\n",
    "        \"max_depth\" : int(max_depth), # default 6\n",
    "        \"subsample\" : subsample, # default 1\n",
    "        \"colsample_bytree\" : colsample_bytree, # default 1\n",
    "\n",
    "        'gamma': gamma,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'max_delta_step': max_delta_step,\n",
    "        'n_estimators': n_estimators,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'max_features': max_features    \n",
    "\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_xgb(params, train_1, y_1, test_1, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo=BayesianOptimization(xgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    1 | 27m07s | \u001b[35m   0.51771\u001b[0m | \u001b[32m            0.7050\u001b[0m | \u001b[32m   6.9771\u001b[0m | \u001b[32m         0.1239\u001b[0m | \u001b[32m          9.4892\u001b[0m | \u001b[32m     6.6818\u001b[0m | \u001b[32m        0.4071\u001b[0m | \u001b[32m            5.7733\u001b[0m | \u001b[32m            13.9950\u001b[0m | \u001b[32m       20.4022\u001b[0m | \u001b[32m     0.5829\u001b[0m | \n",
      "    2 | 14m44s | \u001b[35m   0.52164\u001b[0m | \u001b[32m            0.9858\u001b[0m | \u001b[32m   3.2518\u001b[0m | \u001b[32m         0.1789\u001b[0m | \u001b[32m          4.3073\u001b[0m | \u001b[32m     7.1484\u001b[0m | \u001b[32m        0.7845\u001b[0m | \u001b[32m            6.8698\u001b[0m | \u001b[32m            10.9129\u001b[0m | \u001b[32m       10.3650\u001b[0m | \u001b[32m     0.5045\u001b[0m | \n",
      "    3 | 37m38s |    0.51685 |             0.8255 |    8.8437 |          0.1651 |           7.6349 |      9.1092 |         0.9688 |             9.7978 |              4.9563 |        16.5245 |      0.5383 | \n",
      "    4 | 05m57s |    0.52005 |             0.6660 |    1.5958 |          0.2863 |           0.7153 |      6.1809 |         0.7127 |             3.0230 |              7.9271 |        20.6075 |      0.7328 | \n",
      "    5 | 10m02s |    0.51366 |             0.7340 |    6.4580 |          0.2557 |           5.2078 |      4.3322 |         0.5238 |            10.8191 |             10.8189 |        22.2838 |      0.9161 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    6 | 23m20s |    0.51610 |             0.8507 |    9.6919 |          0.1284 |           1.1047 |      7.8177 |         0.3229 |             0.1478 |             17.6797 |        10.0159 |      0.9181 | \n",
      "    7 | 10m49s |    0.51978 |             0.9861 |    1.1879 |          0.2175 |           9.1743 |      4.6215 |         0.9418 |             0.1079 |              2.5651 |        11.2181 |      0.6134 | \n",
      "    8 | 03m28s |    0.51829 |             0.6506 |    0.0715 |          0.2318 |           9.1756 |      9.9481 |         0.6155 |             1.8924 |             19.7891 |        17.4672 |      0.8672 | \n",
      "    9 | 03m43s |    0.51723 |             0.4175 |    0.8937 |          0.2314 |           1.0272 |      7.8439 |         0.1349 |            17.0277 |              2.5103 |        10.1525 |      0.5137 | \n",
      "   10 | 04m52s |    0.51540 |             0.6054 |    0.0940 |          0.2856 |           0.1390 |      7.3886 |         0.9682 |            19.4506 |             19.8471 |        13.4855 |      0.4003 | \n",
      "   11 | 07m44s |    0.51857 |             0.9464 |    1.3224 |          0.1436 |           3.3234 |      9.7622 |         0.1111 |             2.4593 |              5.7577 |        12.7724 |      0.4586 | \n",
      "   12 | 15m50s |    0.52070 |             0.7024 |    0.2335 |          0.0973 |           2.5165 |      4.2306 |         0.7948 |             0.0976 |             19.2909 |        21.0074 |      0.4707 | \n",
      "   13 | 10m54s |    0.51964 |             0.4166 |    0.2130 |          0.0851 |           1.0696 |      4.4505 |         0.9797 |             3.9259 |             11.0805 |        10.5509 |      0.4401 | \n",
      "   14 | 08m39s |    0.51790 |             0.9038 |    0.5157 |          0.1915 |           9.4627 |      4.0296 |         0.6587 |            18.9487 |             18.5334 |        10.1467 |      0.4320 | \n",
      "   15 | 06m41s |    0.51718 |             0.9824 |    0.3703 |          0.1463 |           9.6226 |      9.8614 |         0.9269 |             1.6850 |              3.2018 |        23.6409 |      0.4628 | \n",
      "   16 | 21m59s |    0.51123 |             0.5689 |    8.6535 |          0.2685 |           0.1358 |      9.8140 |         0.7085 |             2.1155 |             18.6773 |        24.9329 |      0.4303 | \n",
      "   17 | 18m08s |    0.51025 |             0.9998 |    9.2252 |          0.0909 |           8.5279 |      4.6096 |         0.9819 |             0.4848 |             19.8418 |        10.4684 |      0.9160 | \n",
      "   18 | 03m08s |    0.52064 |             0.4694 |    0.2172 |          0.1852 |           9.4691 |      9.8661 |         0.6970 |            17.7468 |              8.6979 |        10.8965 |      0.9672 | \n",
      "   19 | 34m23s |    0.51464 |             0.8446 |    9.7360 |          0.1628 |           0.4873 |      9.7540 |         0.4143 |            19.9028 |             14.5490 |        10.4419 |      0.4745 | \n",
      "   20 | 12m45s |    0.51015 |             0.6266 |    9.9817 |          0.0829 |           9.5504 |      4.3918 |         0.2594 |             2.3720 |              2.0967 |        11.2412 |      0.8799 | \n"
     ]
    }
   ],
   "source": [
    "xgb_bo.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maximum value: 0.521641\n",
      "Best parameters:  {'max_depth': 7.1484196343297413, 'learning_rate': 0.17887629868730259, 'subsample': 0.50453440255149851, 'colsample_bytree': 0.98576451058596715, 'gamma': 3.2517796225471769, 'min_child_weight': 6.8697889345516128, 'max_delta_step': 4.3072885672804917, 'n_estimators': 10.364974451950365, 'min_samples_split': 10.91289701587583, 'max_features': 0.78454358066050689}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % xgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', xgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Velification__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting xgboost...\n",
      "fold cv 0 rmsle score is 0.480301\n",
      "fold cv 1 rmsle score is 0.478021\n",
      "fold cv 2 rmsle score is 0.476746\n",
      "fold cv 3 rmsle score is 0.478559\n",
      "fold cv 4 rmsle score is 0.478631\n",
      "fold cv 5 rmsle score is 0.478513\n",
      "fold cv 6 rmsle score is 0.482232\n",
      "fold cv 7 rmsle score is 0.479451\n",
      "fold cv 8 rmsle score is 0.473449\n",
      "fold cv 9 rmsle score is 0.477626\n",
      "cv rmsle score is 0.478359\n",
      "it takes 930.909 seconds to perform cross validation\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "\n",
    "    'max_depth': int(7.1484196343297413), 'learning_rate': 0.17887629868730259, 'subsample': 0.50453440255149851, 'colsample_bytree': 0.98576451058596715, 'gamma': 3.2517796225471769, 'min_child_weight': 6.8697889345516128, 'max_delta_step': 4.3072885672804917, 'n_estimators': 10.364974451950365, 'min_samples_split': 10.91289701587583, 'max_features': 0.78454358066050689\n",
    "}\n",
    "print(\"Starting xgboost...\")\n",
    "outcomes=cross_validate_xgb(xgb_params, train_1, y_1, test_1, kf, verbose_eval=False)\n",
    "\n",
    "xgb_cv=outcomes[0]\n",
    "xgb_train_1_pred=outcomes[1]\n",
    "xgb_test_1_pred=outcomes[2]\n",
    "\n",
    "xgb_train_1_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_train_1_pred)\n",
    "xgb_test_1_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_test_1_pred)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling based on train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cross validation with xgboost__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-rmse:2.3282\tval-rmse:2.32892\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.523935\tval-rmse:0.528063\n",
      "[100]\ttrain-rmse:0.48115\tval-rmse:0.487976\n",
      "[150]\ttrain-rmse:0.474884\tval-rmse:0.483455\n",
      "[200]\ttrain-rmse:0.471059\tval-rmse:0.481107\n",
      "[250]\ttrain-rmse:0.46766\tval-rmse:0.479155\n",
      "[300]\ttrain-rmse:0.465053\tval-rmse:0.478059\n",
      "[350]\ttrain-rmse:0.462606\tval-rmse:0.477149\n",
      "[400]\ttrain-rmse:0.460382\tval-rmse:0.476361\n",
      "[450]\ttrain-rmse:0.458584\tval-rmse:0.475764\n",
      "[500]\ttrain-rmse:0.456891\tval-rmse:0.475273\n",
      "[550]\ttrain-rmse:0.455432\tval-rmse:0.474972\n",
      "[600]\ttrain-rmse:0.4541\tval-rmse:0.47464\n",
      "[650]\ttrain-rmse:0.452775\tval-rmse:0.474391\n",
      "[700]\ttrain-rmse:0.451427\tval-rmse:0.474113\n",
      "[750]\ttrain-rmse:0.450118\tval-rmse:0.473927\n",
      "[800]\ttrain-rmse:0.448668\tval-rmse:0.473666\n",
      "[850]\ttrain-rmse:0.447437\tval-rmse:0.4735\n",
      "[900]\ttrain-rmse:0.446173\tval-rmse:0.47339\n",
      "[950]\ttrain-rmse:0.444979\tval-rmse:0.473245\n",
      "[1000]\ttrain-rmse:0.443848\tval-rmse:0.473171\n",
      "[1050]\ttrain-rmse:0.442688\tval-rmse:0.4731\n",
      "[1100]\ttrain-rmse:0.441665\tval-rmse:0.473117\n",
      "[1150]\ttrain-rmse:0.440449\tval-rmse:0.472987\n",
      "[1200]\ttrain-rmse:0.439238\tval-rmse:0.472839\n",
      "[1250]\ttrain-rmse:0.438283\tval-rmse:0.472813\n",
      "[1300]\ttrain-rmse:0.437308\tval-rmse:0.47289\n",
      "Stopping. Best iteration:\n",
      "[1232]\ttrain-rmse:0.438669\tval-rmse:0.472793\n",
      "\n",
      "[0]\ttrain-rmse:2.32755\tval-rmse:2.3305\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.523289\tval-rmse:0.529805\n",
      "[100]\ttrain-rmse:0.480501\tval-rmse:0.489526\n",
      "[150]\ttrain-rmse:0.473903\tval-rmse:0.485083\n",
      "[200]\ttrain-rmse:0.469669\tval-rmse:0.482704\n",
      "[250]\ttrain-rmse:0.466401\tval-rmse:0.481171\n",
      "[300]\ttrain-rmse:0.463987\tval-rmse:0.480191\n",
      "[350]\ttrain-rmse:0.462584\tval-rmse:0.479731\n",
      "[400]\ttrain-rmse:0.461306\tval-rmse:0.479397\n",
      "[450]\ttrain-rmse:0.460027\tval-rmse:0.478968\n",
      "[500]\ttrain-rmse:0.459133\tval-rmse:0.4787\n",
      "[550]\ttrain-rmse:0.458239\tval-rmse:0.478402\n",
      "[600]\ttrain-rmse:0.457346\tval-rmse:0.478155\n",
      "[650]\ttrain-rmse:0.45649\tval-rmse:0.477934\n",
      "[700]\ttrain-rmse:0.455995\tval-rmse:0.477829\n",
      "[750]\ttrain-rmse:0.455206\tval-rmse:0.47764\n",
      "[800]\ttrain-rmse:0.454377\tval-rmse:0.477338\n",
      "[850]\ttrain-rmse:0.453646\tval-rmse:0.477195\n",
      "[900]\ttrain-rmse:0.452853\tval-rmse:0.477\n",
      "[950]\ttrain-rmse:0.452266\tval-rmse:0.476931\n",
      "[1000]\ttrain-rmse:0.451589\tval-rmse:0.476807\n",
      "[1050]\ttrain-rmse:0.450822\tval-rmse:0.476704\n",
      "[1100]\ttrain-rmse:0.450118\tval-rmse:0.476579\n",
      "[1150]\ttrain-rmse:0.449228\tval-rmse:0.476407\n",
      "[1200]\ttrain-rmse:0.448489\tval-rmse:0.476291\n",
      "[1250]\ttrain-rmse:0.447784\tval-rmse:0.476174\n",
      "[1300]\ttrain-rmse:0.447059\tval-rmse:0.476095\n",
      "[1350]\ttrain-rmse:0.446439\tval-rmse:0.47608\n",
      "[1400]\ttrain-rmse:0.445785\tval-rmse:0.475977\n",
      "[1450]\ttrain-rmse:0.445292\tval-rmse:0.475926\n",
      "[1500]\ttrain-rmse:0.444588\tval-rmse:0.475861\n",
      "[1550]\ttrain-rmse:0.443931\tval-rmse:0.47579\n",
      "[1600]\ttrain-rmse:0.443251\tval-rmse:0.475712\n",
      "[1650]\ttrain-rmse:0.442812\tval-rmse:0.475706\n",
      "[1700]\ttrain-rmse:0.442191\tval-rmse:0.475726\n",
      "[1750]\ttrain-rmse:0.441717\tval-rmse:0.475732\n",
      "Stopping. Best iteration:\n",
      "[1663]\ttrain-rmse:0.442565\tval-rmse:0.475693\n",
      "\n",
      "[0]\ttrain-rmse:2.32972\tval-rmse:2.32604\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.525858\tval-rmse:0.524627\n",
      "[100]\ttrain-rmse:0.48293\tval-rmse:0.484655\n",
      "[150]\ttrain-rmse:0.476078\tval-rmse:0.480226\n",
      "[200]\ttrain-rmse:0.471422\tval-rmse:0.477746\n",
      "[250]\ttrain-rmse:0.467933\tval-rmse:0.476289\n",
      "[300]\ttrain-rmse:0.465237\tval-rmse:0.475265\n",
      "[350]\ttrain-rmse:0.462733\tval-rmse:0.474328\n",
      "[400]\ttrain-rmse:0.46041\tval-rmse:0.473534\n",
      "[450]\ttrain-rmse:0.458467\tval-rmse:0.473027\n",
      "[500]\ttrain-rmse:0.456548\tval-rmse:0.472482\n",
      "[550]\ttrain-rmse:0.454773\tval-rmse:0.471918\n",
      "[600]\ttrain-rmse:0.452992\tval-rmse:0.471655\n",
      "[650]\ttrain-rmse:0.451472\tval-rmse:0.471413\n",
      "[700]\ttrain-rmse:0.449903\tval-rmse:0.471162\n",
      "[750]\ttrain-rmse:0.448382\tval-rmse:0.470891\n",
      "[800]\ttrain-rmse:0.44696\tval-rmse:0.470671\n",
      "[850]\ttrain-rmse:0.44548\tval-rmse:0.470495\n",
      "[900]\ttrain-rmse:0.44426\tval-rmse:0.47049\n",
      "[950]\ttrain-rmse:0.442978\tval-rmse:0.470444\n",
      "[1000]\ttrain-rmse:0.441682\tval-rmse:0.470339\n",
      "[1050]\ttrain-rmse:0.440401\tval-rmse:0.470258\n",
      "[1100]\ttrain-rmse:0.439087\tval-rmse:0.470217\n",
      "[1150]\ttrain-rmse:0.43792\tval-rmse:0.470342\n",
      "Stopping. Best iteration:\n",
      "[1089]\ttrain-rmse:0.439414\tval-rmse:0.470189\n",
      "\n",
      "cv rmsle score is 0.472899\n"
     ]
    }
   ],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    #\"num_class\" : 3,\n",
    "    #\"tree_method\" : \"hist\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.05,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "cv_score =cross_validate_xgb(xgb_params, train_2, y_2, test_2, kf, verbose=False, verbose_eval=50, scoreonly=True)\n",
    "\n",
    "print('cv rmsle score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayesian Optimsation - Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth':(4,10),\n",
    "        'learning_rate':(0.05,0.3),\n",
    "        'subsample': (0.4, 1),\n",
    "        'colsample_bytree': (0.4, 1),\n",
    "        'gamma': (0.001, 10.0),\n",
    "        'min_child_weight': (0, 20),\n",
    "        'max_delta_step': (0, 10),\n",
    "        'n_estimators': (10, 25),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'max_features': (0.1, 0.999)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(xgb_wrapper)\n",
    "def xgbcv_func(max_depth, learning_rate, subsample, \n",
    "               colsample_bytree, gamma, min_child_weight, \n",
    "               max_delta_step, n_estimators, \n",
    "               min_samples_split, max_features,nthread=4, seed=0):\n",
    "    params = {\n",
    "        \"objective\" : \"reg:linear\",\n",
    "        #\"num_class\" : 3,\n",
    "        #\"tree_method\" : \"hist\",\n",
    "        \"eval_metric\" : \"rmse\",\n",
    "        \"nthread\": nthread,\n",
    "        \"seed\" : 0,\n",
    "        'silent': 1,\n",
    "\n",
    "        \"eta\":learning_rate,  # default 0.3\n",
    "        \"max_depth\" : int(max_depth), # default 6\n",
    "        \"subsample\" : subsample, # default 1\n",
    "        \"colsample_bytree\" : colsample_bytree, # default 1\n",
    "\n",
    "        'gamma': gamma,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'max_delta_step': max_delta_step,\n",
    "        'n_estimators': n_estimators,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'max_features': max_features    \n",
    "\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_xgb(params, train_2, y_2, test_2, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo=BayesianOptimization(xgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    1 | 24m28s | \u001b[35m   0.52110\u001b[0m | \u001b[32m            0.5915\u001b[0m | \u001b[32m   8.9828\u001b[0m | \u001b[32m         0.2511\u001b[0m | \u001b[32m          1.3957\u001b[0m | \u001b[32m     9.1532\u001b[0m | \u001b[32m        0.2781\u001b[0m | \u001b[32m            8.8760\u001b[0m | \u001b[32m            18.5186\u001b[0m | \u001b[32m       23.5401\u001b[0m | \u001b[32m     0.4587\u001b[0m | \n",
      "    2 | 19m11s | \u001b[35m   0.52532\u001b[0m | \u001b[32m            0.7042\u001b[0m | \u001b[32m   4.3236\u001b[0m | \u001b[32m         0.2002\u001b[0m | \u001b[32m          6.2094\u001b[0m | \u001b[32m     4.8409\u001b[0m | \u001b[32m        0.6829\u001b[0m | \u001b[32m           19.6131\u001b[0m | \u001b[32m            12.3590\u001b[0m | \u001b[32m       24.6200\u001b[0m | \u001b[32m     0.5929\u001b[0m | \n",
      "    3 | 13m21s | \u001b[35m   0.52736\u001b[0m | \u001b[32m            0.8005\u001b[0m | \u001b[32m   2.5747\u001b[0m | \u001b[32m         0.1931\u001b[0m | \u001b[32m          9.9351\u001b[0m | \u001b[32m     7.8605\u001b[0m | \u001b[32m        0.9780\u001b[0m | \u001b[32m            1.4375\u001b[0m | \u001b[32m             8.9850\u001b[0m | \u001b[32m       18.2491\u001b[0m | \u001b[32m     0.9873\u001b[0m | \n",
      "    4 | 43m26s |    0.52484 |             0.6982 |    4.6554 |          0.1116 |           0.1662 |      4.8457 |         0.8977 |            13.3818 |              9.7513 |        18.7676 |      0.7362 | \n",
      "    5 | 11m08s |    0.52135 |             0.6259 |    5.5305 |          0.2858 |           4.5093 |      5.1371 |         0.4186 |             1.8187 |              7.6763 |        18.0344 |      0.4081 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    6 | 19m29s | \u001b[35m   0.52951\u001b[0m | \u001b[32m            0.7009\u001b[0m | \u001b[32m   1.3228\u001b[0m | \u001b[32m         0.1067\u001b[0m | \u001b[32m          9.6928\u001b[0m | \u001b[32m     6.8961\u001b[0m | \u001b[32m        0.2645\u001b[0m | \u001b[32m           19.0867\u001b[0m | \u001b[32m            19.3971\u001b[0m | \u001b[32m       10.6433\u001b[0m | \u001b[32m     0.9398\u001b[0m | \n",
      "    7 | 07m51s |    0.52661 |             0.4582 |    0.2160 |          0.0817 |           5.4036 |      9.9862 |         0.3869 |            19.9900 |              2.0393 |        10.9403 |      0.5621 | \n",
      "    8 | 05m08s |    0.52253 |             0.6751 |    0.0407 |          0.2399 |           1.2552 |      7.7269 |         0.5796 |             0.0636 |             19.5935 |        24.7907 |      0.5533 | \n",
      "    9 | 06m45s |    0.52444 |             0.6342 |    0.6486 |          0.1250 |           9.6906 |      9.1758 |         0.9472 |            19.4642 |             15.6901 |        11.0394 |      0.4296 | \n",
      "   10 | 12m05s |    0.51986 |             0.8185 |    7.9557 |          0.2059 |           9.7445 |      4.1596 |         0.8217 |            19.5064 |              6.4109 |        10.1564 |      0.9376 | \n",
      "   11 | 10m19s |    0.52696 |             0.9146 |    1.0837 |          0.1529 |           1.7016 |      8.7262 |         0.1708 |             4.1831 |              2.1585 |        24.7577 |      0.8173 | \n",
      "   12 | 13m16s |    0.52503 |             0.6720 |    0.0634 |          0.1690 |           0.1311 |      9.3876 |         0.1936 |             8.8604 |             16.9432 |        10.1600 |      0.9848 | \n",
      "   13 | 18m07s |    0.52817 |             0.4263 |    0.4511 |          0.0827 |           9.3451 |      4.3524 |         0.3710 |             7.3013 |              2.7160 |        24.9714 |      0.9907 | \n",
      "   14 | 21m29s |    0.52801 |             0.9029 |    1.0289 |          0.1860 |           0.4325 |      4.7136 |         0.5993 |            19.8875 |             19.6020 |        24.9106 |      0.8722 | \n",
      "   15 | 101m37s |    0.52817 |             0.9110 |    0.2546 |          0.1315 |           9.0498 |      4.3341 |         0.1330 |             5.7169 |             19.6793 |        19.1672 |      0.9070 | \n",
      "   16 | 03m32s |    0.52357 |             0.4739 |    0.4821 |          0.2732 |           9.1536 |      9.4348 |         0.1248 |            16.7939 |             14.9442 |        24.3676 |      0.9964 | \n",
      "   17 | 15m51s |    0.52108 |             0.5676 |    9.2612 |          0.2535 |           0.1772 |      8.4963 |         0.2544 |            19.2947 |              2.4386 |        24.4323 |      0.9933 | \n",
      "   18 | 09m35s |    0.52562 |             0.7021 |    0.2003 |          0.1427 |           0.2905 |      9.7024 |         0.5828 |             0.6992 |              2.0498 |        10.7402 |      0.8494 | \n",
      "   19 | 26m26s |    0.52048 |             0.7910 |    8.8753 |          0.1083 |           8.1795 |      4.8355 |         0.8229 |             0.6029 |             18.8279 |        24.8180 |      0.7824 | \n",
      "   20 | 55m38s |    0.52420 |             0.9464 |    0.0337 |          0.0513 |           9.2421 |      4.0080 |         0.1148 |            18.0803 |              3.7623 |        13.7242 |      0.9966 | \n"
     ]
    }
   ],
   "source": [
    "xgb_bo.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maximum value: 0.529510\n",
      "Best parameters:  {'max_depth': 6.8960955944176909, 'learning_rate': 0.1067125144540214, 'subsample': 0.93984945674997611, 'colsample_bytree': 0.70093842006487073, 'gamma': 1.3228005298535754, 'min_child_weight': 19.086672690291731, 'max_delta_step': 9.6928478134941152, 'n_estimators': 10.643264156851325, 'min_samples_split': 19.397089382225694, 'max_features': 0.26451327557648185}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % xgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', xgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Velification__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting xgboost...\n",
      "fold cv 0 rmsle score is 0.477364\n",
      "fold cv 1 rmsle score is 0.473759\n",
      "fold cv 2 rmsle score is 0.470892\n",
      "fold cv 3 rmsle score is 0.471985\n",
      "fold cv 4 rmsle score is 0.463737\n",
      "fold cv 5 rmsle score is 0.467912\n",
      "fold cv 6 rmsle score is 0.472384\n",
      "fold cv 7 rmsle score is 0.472876\n",
      "fold cv 8 rmsle score is 0.465027\n",
      "fold cv 9 rmsle score is 0.468725\n",
      "cv rmsle score is 0.470490\n",
      "it takes 2023.403 seconds to perform cross validation\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': int(6.8960955944176909), 'learning_rate': 0.1067125144540214, 'subsample': 0.93984945674997611, 'colsample_bytree': 0.70093842006487073, 'gamma': 1.3228005298535754, 'min_child_weight': 19.086672690291731, 'max_delta_step': 9.6928478134941152, 'n_estimators': 10.643264156851325, 'min_samples_split': 19.397089382225694, 'max_features': 0.26451327557648185\n",
    "}\n",
    "print(\"Starting xgboost...\")\n",
    "outcomes=cross_validate_xgb(xgb_params, train_2, y_2, test_2, kf, verbose_eval=False)\n",
    "\n",
    "xgb_cv=outcomes[0]\n",
    "xgb_train_2_pred=outcomes[1]\n",
    "xgb_test_2_pred=outcomes[2]\n",
    "\n",
    "xgb_train_2_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_train_2_pred)\n",
    "xgb_test_2_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_test_2_pred)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total train: (252108, 81)\n",
    "# 316 for period 1: (47699, 81)\n",
    "# 316 for period 1 & 2: (126700, 81) <=== To be used\n",
    "# 513 for period 2 (125408, 81)\n",
    "# 316 for period 1 + 513 for period 2 (173107, 81) <=== To be used\n",
    "\n",
    "# Total test: (32019, 81)\n",
    "# test with 316(12207, 81)\n",
    "# test with 513(19812, 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainが正しく整理できていない。\n",
    "\n",
    "_tra_316 = tra_316_all.drop([\"visitors\"],axis=1)\n",
    "#_tra_513 = tra_513_316.drop([\"visitors\"],axis=1)\n",
    "_tra_513 = tra_513.drop([\"visitors\"],axis=1)\n",
    "\n",
    "_tes_316 = tes_316.drop([\"visitors\"],axis=1)\n",
    "_tes_513 = tes_513.drop([\"visitors\"],axis=1)\n",
    "\n",
    "# 316 for period 1 & 2: (126700, 81) <=== To be used\n",
    "lv1_xgb_train_1_pred = xgb_train_1_pred_df.copy()\n",
    "lv1_xgb_train_1_pred.to_csv('lv1_xgb_train_1_pred.csv', index=False)\n",
    "\n",
    "lv1_xgb_test_1_pred = xgb_test_1_pred_df.copy()\n",
    "lv1_xgb_test_1_pred.to_csv('lv1_xgb_test_1_pred.csv', index=False)\n",
    "\n",
    "_tra_316 = pd.concat([_tra_316,lv1_xgb_train_1_pred], axis=1)\n",
    "_tes_316 = pd.concat([_tes_316,lv1_xgb_test_1_pred], axis=1)\n",
    "\n",
    "# 316 for period 1 + 513 for period 2 (173107, 81) <=== To be used\n",
    "lv1_xgb_train_2_pred = xgb_train_2_pred_df.copy()\n",
    "lv1_xgb_train_2_pred.to_csv('lv1_xgb_train_2_pred.csv', index=False)\n",
    "\n",
    "lv1_xgb_test_2_pred = xgb_test_2_pred_df.copy()\n",
    "lv1_xgb_test_2_pred.to_csv('lv1_xgb_test_2_pred.csv', index=False)\n",
    "\n",
    "#_tra_513 = pd.concat([_tra_513,lv1_xgb_train_2_pred], axis=1)\n",
    "_tra_513 = pd.concat([_tra_513,lv1_xgb_train_2_pred], axis=1,join_axes=[_tra_513.index])\n",
    "_tes_513 = pd.concat([_tes_513,lv1_xgb_test_2_pred], axis=1)\n",
    "\n",
    "# Merge\n",
    "xgb_train_pred_df = pd.concat([_tra_316,_tra_513])\n",
    "xgb_test_pred_df = pd.concat([_tes_316,_tes_513])\n",
    "\n",
    "lv1_xgb_train_input = xgb_train_pred_df[['id','visitors']].copy()\n",
    "lv1_xgb_test_input = xgb_test_pred_df[['id','visitors']].copy()\n",
    "\n",
    "lv1_xgb_train_input = lv1_xgb_train_input.sort_values(by=['id'])\n",
    "lv1_xgb_test_input = lv1_xgb_test_input.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126700, 81)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin[train_origin['air_store_id'].isin(id)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125408, 81)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin[~train_origin['air_store_id'].isin(id)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin =  pd.concat([tra_store_316_all,tra_store_513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252108, 81)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin[\"pred_visitors\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252108, 82)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_index = train_origin[train_origin['air_store_id'].isin(id)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     0,      1,      2,      3,      4,      5,      6,      7,\n",
       "                 8,      9,\n",
       "            ...\n",
       "            251864, 251865, 251866, 251867, 251868, 251869, 251870, 251871,\n",
       "            251872, 251873],\n",
       "           dtype='int64', length=126700)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin_ = np.zeros((train_origin.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252108,)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (126700,1) could not be broadcast to indexing result of shape (126700,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-0c298beda012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_origin_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtra_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlv1_xgb_train_1_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (126700,1) could not be broadcast to indexing result of shape (126700,)"
     ]
    }
   ],
   "source": [
    "train_origin_[tra_index] = lv1_xgb_train_1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin[train_origin['air_store_id'].isin(id)].pred_visitors = lv1_xgb_train_1_pred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252108, 82)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    252108\n",
       "Name: pred_visitors, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin.pred_visitors.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin_513 = pd.concat([tra_store_316_6,tra_store_513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173107, 81)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin_513.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin_513[\"pred_visitors\"] = lv1_xgb_train_2_pred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125408,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin_513[~train_origin_513['air_store_id'].isin(id)].pred_visitors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_val = train_origin_513[~train_origin_513['air_store_id'].isin(id)].pred_visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125408,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin[~train_origin['air_store_id'].isin(id)][\"pred_visitors\"] = temp_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252108, 81)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id 失敗。あとで整理。\n",
    "lv1_xgb_train_input = lv1_xgb_train_input.drop([\"id\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good luck :)\n"
     ]
    }
   ],
   "source": [
    "lv1_xgb_train_input.to_csv('lv1_xgb_train_pred.csv', index=False)\n",
    "lv1_xgb_test_input.to_csv('lv1_xgb_test_pred.csv', index=False)\n",
    "\n",
    "lv1_xgb_test_input.to_csv('submission_rs_recruit_v14_xgb_linear_fe_suzukiry_01.csv', index=False)\n",
    "\n",
    "print('Good luck :)')\n",
    "\n",
    "#fe\n",
    "#xgboost\n",
    "#Bopt\n",
    "#LB NOT SUBMITTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Consider weight__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v14_xgb_linear_fe_suzukiry_02.csv', index=False)\n",
    "\n",
    "# fe\n",
    "# xgb: linear\n",
    "# Bopt\n",
    "# weight\n",
    "# train data separation\n",
    "# LB 0.480"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
