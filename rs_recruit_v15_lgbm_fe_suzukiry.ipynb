{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LightGBM with Bayesian Optimsation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thanks to Shinji Suzuki\n",
    "\n",
    "# OS\n",
    "import glob, re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# data science tool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# machine learning\n",
    "from sklearn import *\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# データの読み込み\n",
    "# 事前にcalendar_dateをvisit_dataに変更しています。airとhpgで同じことですが、別名で使用されているようです。\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "}\n",
    "\n",
    "# それぞれのデータをマージするために、まずは、relation用のものをマージします\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how = 'inner', on = ['hpg_store_id'])\n",
    "\n",
    "for df in ['ar', 'hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r:(r['visit_datetime']- r['reserve_datetime']).days, axis = 1)\n",
    "    tmp1 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how = 'inner', on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'dow':[i]*len(unique_stores)}) for i in range(7)], axis =0, ignore_index = True).reset_index(drop = True) \n",
    "\n",
    "#曜日だけでなく、月も追加\n",
    "stores_m = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'month':[i]*len(unique_stores)}) for i in range(1,13)], axis =0, ignore_index = True).reset_index(drop = True)\n",
    "stores = pd.merge(stores_m, stores,on=('air_store_id'), how='left')\n",
    "\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].min().rename(columns = {'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].median().rename(columns = {'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].max().rename(columns = {'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].count().rename(columns = {'visitors':'count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "\n",
    "#曜日だけでなく、ID×月も追加\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].min().rename(columns = {'visitors':'m_min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'m_mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].median().rename(columns = {'visitors':'m_median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].max().rename(columns = {'visitors':'m_max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].count().rename(columns = {'visitors':'m_count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "\n",
    "stores = pd.merge(stores, data['as'], how= \"left\", on = ['air_store_id'])\n",
    "\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/', ' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-', ' ')))\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name' + str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "    stores['air_area_name' + str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "#土日フラグ(day_of_week_1)と、休日前(holi_2)フラグを追加\n",
    "data['hol']['day_of_week_1']= data['hol']['day_of_week'].replace(['Saturday', 'Sunday','Monday','Tuesday','Wednesday','Thursday','Friday'],['1', '1','0','0','0','0','0']).astype('int')\n",
    "data['hol']['holi_2'] = data['hol'][['holiday_flg', 'day_of_week_1']].sum(axis = 1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].apply( lambda x: 0 if x < 1 else 1 )\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].shift(-1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].fillna(1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].astype('int')\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how ='left', on = ['visit_date'])\n",
    "test = pd.merge(data['tes'], data['hol'], how ='left', on = ['visit_date'])\n",
    "\n",
    "#曜日と月でmerge\n",
    "train = pd.merge(train, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "test = pd.merge(test, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "\n",
    "#ID×休日前でのvisitorsの平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].min().rename(columns = {'visitors':'h_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'h_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].median().rename(columns = {'visitors':'h_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].max().rename(columns = {'visitors':'h_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].count().rename(columns = {'visitors':'h_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1) \n",
    "\n",
    "train['total_reserve_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserve_mean'] = (train['rv2_x'] + train['rv2_y'])/2\n",
    "train['total_reserve_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y'])/2\n",
    "\n",
    "test['total_reserve_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserve_mean'] = (test['rv2_x'] + test['rv2_y'])/2\n",
    "test['total_reserve_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y'])/2\n",
    "\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2']= lbl.fit_transform(test['air_store_id'])\n",
    "\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date', 'visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "\n",
    "all_data = pd.concat([train, test]) \n",
    "\n",
    "#指数移動平均の追加。これはなくても良いかも\n",
    "#https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46179#266344\n",
    "#def calc_shifted_ewm(series, alpha, adjust=True):\n",
    "#    return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n",
    "\n",
    "#train['ewm'] = train.groupby(['air_store_id', 'dow']).apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1)).sort_index(level=['air_store_id']).values\n",
    "\n",
    "#以下、気象データの追加\n",
    "df_air_store_weather_station = pd.read_csv('../../../mltestdata/05_recruit/air_store_info_with_nearest_active_station.csv')\n",
    "\n",
    "cols = ['air_store_id', 'station_id', 'station_latitude', 'station_longitude', 'station_vincenty', 'station_great_circle']\n",
    "all_data = pd.merge(all_data, df_air_store_weather_station[cols], on='air_store_id', how='left')\n",
    "\n",
    "combine = all_data\n",
    "filenames = []\n",
    "df_weather = None\n",
    "for station_id in combine['station_id'].unique():\n",
    "    fn = f\"../../../mltestdata/05_recruit/1-1-16_5-31-17_Weather/{station_id}.csv\"\n",
    "    if not fn in filenames:\n",
    "        df = pd.read_csv(fn)\n",
    "        df['station_id'] = station_id\n",
    "        if df_weather is None:\n",
    "            df_weather = df\n",
    "        else:\n",
    "            df_weather = pd.concat([df_weather, df])\n",
    "        del df\n",
    "\n",
    "        filenames.append(fn)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#欠損値を平均で穴埋め（median, ffillで試すも特に差は出なかった）\n",
    "df_weather = df_weather.fillna(df_weather.mean())\n",
    "\n",
    "df_weather = df_weather.rename(columns={'calendar_date': 'visit_date'})\n",
    "\n",
    "df_weather['visit_date'] = pd.to_datetime(df_weather['visit_date'])\n",
    "df_weather['visit_date'] = df_weather['visit_date'].dt.date\n",
    "\n",
    "#なんとなく対数化\n",
    "df_weather['precipitation'] = np.log1p(df_weather['precipitation'])\n",
    "\n",
    "#使いそうなデータだけ結合（その他の気象データは試していません。特に意味はなし）\n",
    "cols = ['station_id', \n",
    "    'visit_date', \n",
    "    'precipitation', \n",
    "    'hours_sunlight',\n",
    "    'avg_temperature',\n",
    "    'high_temperature',\n",
    "    'low_temperature']\n",
    "\n",
    "combine = pd.merge(combine, df_weather[cols], on=['station_id', 'visit_date'], how='left')\n",
    "\n",
    "#降水量をカテゴリ化\n",
    "def simplify_pre(df):\n",
    "    df.precipitation = df.precipitation.fillna(0)\n",
    "    bins = ( -1, 0.01, 2,  5)\n",
    "    group_names = ['1', '2', '3']\n",
    "    categories = pd.cut(df.precipitation, bins, labels=group_names)\n",
    "    df.precipitation = categories\n",
    "    return df\n",
    "\n",
    "combine = simplify_pre(combine) \n",
    "all_data = combine \n",
    "\n",
    "################################## modified on 1-Feb\n",
    "#不要そうなデータを削除\n",
    "#drop_col =['station_id', 'station_latitude','station_longitude','station_vincenty', 'station_great_circle','hours_sunlight','high_temperature','low_temperature']\n",
    "drop_col =['hours_sunlight','high_temperature','low_temperature']\n",
    "all_data = all_data.drop(drop_col, axis = 1)\n",
    "\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "\n",
    "################################## modified on 1-Feb\n",
    "#Encode station_id\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['station_id'] = lbl.fit_transform(train['station_id']) \n",
    "test['station_id'] = lbl.fit_transform(test['station_id'])\n",
    "\n",
    "#ID×降水量で平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].min().rename(columns = {'visitors':'p_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'p_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].median().rename(columns = {'visitors':'p_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation',])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].max().rename(columns = {'visitors':'p_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].count().rename(columns = {'visitors':'p_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "#countをLabel Encoder化。なんとなく試してみたら結果が良かった。\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['count_visitors'] = lbl.fit_transform(train['count_visitors']) \n",
    "test['count_visitors']= lbl.fit_transform(test['count_visitors'])\n",
    "train['m_count_visitors'] = lbl.fit_transform(train['m_count_visitors'])\n",
    "test['m_count_visitors']= lbl.fit_transform(test['m_count_visitors'])\n",
    "train['h_count_visitors'] = lbl.fit_transform(train['h_count_visitors'])\n",
    "test['h_count_visitors']= lbl.fit_transform(test['h_count_visitors'])\n",
    "train['p_count_visitors'] = lbl.fit_transform(train['p_count_visitors'])\n",
    "test['p_count_visitors']= lbl.fit_transform(test['p_count_visitors'])\n",
    "\n",
    "# GW flag\n",
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]\n",
    "\n",
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train: (252108, 80)\n",
      "316 for period 1: (47699, 80)\n",
      "316 for period 1 & 2: (126700, 80) <=== To be used\n",
      "513 for period 2 (125408, 80)\n",
      "316 for period 1 + 513 for period 2 (173107, 80) <=== To be used\n",
      "\n",
      "Total test: (32019, 80)\n",
      "test with 316(12207, 80)\n",
      "test with 513(19812, 80)\n"
     ]
    }
   ],
   "source": [
    "tra = train.copy()\n",
    "tes = test.copy()\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "#tra = pd.read_csv('/Users/suzukishinji/kaggle/recluit/air_visit_data.csv')\n",
    "#tes = pd.read_csv('/Users/suzukishinji/kaggle/recluit/sample_submission-3.csv')\n",
    "\n",
    "#countで試してみましたが、「2016年初めからデータはあるものの営業日の少ないお店」と「データは途中からだけど営業日の多い店」が混じるため、2016/1~2016/6に営業していた店と営業していないお店という分け方をしてみました。\n",
    "\n",
    "tra['visit_date'] = pd.to_datetime(tra['visit_date'])\n",
    "tra['visit_date'] = pd.to_datetime(tra['visit_date'])\n",
    "tra['dow'] = tra['visit_date'].dt.dayofweek\n",
    "tra['year'] = tra['visit_date'].dt.year\n",
    "tra['month'] = tra['visit_date'].dt.month\n",
    "tra['visit_date'] = tra['visit_date'].dt.date\n",
    "\n",
    "#2016/1~2016/6に営業しているお店は316店舗でした。\n",
    "year_2016 = tra[tra['year'] == 2016]\n",
    "month_1 = year_2016[year_2016['month'] == 1]\n",
    "month_2 = year_2016[year_2016['month'] == 2]\n",
    "month_3 = year_2016[year_2016['month'] == 3]\n",
    "month_4 = year_2016[year_2016['month'] == 4]\n",
    "month_5 = year_2016[year_2016['month'] == 5]\n",
    "month_6 = year_2016[year_2016['month'] == 6]\n",
    "tra_store_316_6 = pd.concat([month_1,month_2,month_3,month_4,month_5,month_6])\n",
    "\n",
    "id = list(tra_store_316_6['air_store_id'].values.flatten())\n",
    "\n",
    "#trainデータから2016/1~2016/6に営業しているお店の全期間を抜き出したもの。\n",
    "#trainデータから上記を除いたものが513店舗。\n",
    "tra_store_316_all = tra[tra['air_store_id'].isin(id)]\n",
    "tra_store_513 = tra[~tra['air_store_id'].isin(id)] # これはNOTなんだ。\n",
    "\n",
    "tes['visit_date'] = tes['id'].map(lambda x: str(x).split('_')[2])\n",
    "tes['air_store_id'] = tes['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "tes['visit_date'] = pd.to_datetime(tes['visit_date'])\n",
    "tes['dow'] = tes['visit_date'].dt.dayofweek\n",
    "tes['year'] = tes['visit_date'].dt.year\n",
    "tes['month'] = tes['visit_date'].dt.month\n",
    "tes['visit_date'] = tes['visit_date'].dt.date\n",
    "\n",
    "#testデータから2016/1~2016/6に営業しているお店の全期間を抜き出したもの。\n",
    "#testデータから上記を除いたものが513店舗。\n",
    "tes_store_316 = tes[tes['air_store_id'].isin(id)]\n",
    "tes_store_513 = tes[~tes['air_store_id'].isin(id)]\n",
    "\n",
    "#使うのは①2016/1~2016/6の期間のデータ、②2016/1~2016/6に営業していたお店の全期間のデータ、③途中からの513店舗のデータ。\n",
    "#tra_store_316_6[['air_store_id', 'visit_date','visitors']].to_csv('tra_store_316_6.csv', index = False)\n",
    "#tra_store_316_all[['air_store_id', 'visit_date','visitors']].to_csv('tra_store_316_all.csv', index = False)\n",
    "#tra_store_513[['air_store_id', 'visit_date','visitors']].to_csv('tra_store_513.csv', index = False)\n",
    "\n",
    "#使うのは①2016/1~2016/6の期間のデータ、②2016/1~2016/6に営業していたお店の全期間のデータ、③途中からの513店舗のデータ。\n",
    "tra_store_316_6.to_csv('tra_store_316_6.csv', index = False)\n",
    "tra_store_316_all.to_csv('tra_store_316_all.csv', index = False)\n",
    "tra_store_513.to_csv('tra_store_513.csv', index = False)\n",
    "\n",
    "#テストデータは期間で分けずに最初からの316店舗と途中からの513店舗\n",
    "#tes_store_316[['id','visitors']].to_csv('tes_store_316.csv', index = False)\n",
    "#tes_store_513[['id','visitors']].to_csv('tes_store_513.csv', index = False)\n",
    "\n",
    "tes_store_316.to_csv('tes_store_316.csv', index = False)\n",
    "tes_store_513.to_csv('tes_store_513.csv', index = False)\n",
    "\n",
    "tra_316_6 = pd.read_csv('./tra_store_316_6.csv') # 316 for period 1 (first 6 months)\n",
    "tra_316_all = pd.read_csv('./tra_store_316_all.csv') # 316 for period 1 & 2\n",
    "tra_513 = pd.read_csv('./tra_store_513.csv') # 513 for period 2 (expect first 6 months)\n",
    "tes_316 = pd.read_csv('./tes_store_316.csv') # 316 only\n",
    "tes_513 = pd.read_csv('./tes_store_513.csv') # 513 only\n",
    "\n",
    "#出力されているか確認。（①と③はくっつけて使う）\n",
    "print(\"Total train: \"+str(tra.shape))\n",
    "print(\"316 for period 1: \"+str(tra_316_6.shape))\n",
    "print(\"316 for period 1 & 2: \"+str(tra_316_all.shape)+\" <=== To be used\")\n",
    "print(\"513 for period 2 \"+str(tra_513.shape))\n",
    "tra_513_316 = pd.concat([tra_316_6, tra_513], ignore_index=True)\n",
    "print(\"316 for period 1 + 513 for period 2 \"+str(tra_513_316.shape)+\" <=== To be used\"+\"\\n\")\n",
    "print(\"Total test: \"+str(tes.shape))\n",
    "print(\"test with 316\"+str(tes_316.shape))\n",
    "print(\"test with 513\"+str(tes_513.shape))\n",
    "\n",
    "train_1 = tra_316_all.copy()\n",
    "test_1 = tes_316.copy()\n",
    "\n",
    "train_2 = tra_513_316.copy()\n",
    "test_2 = tes_513.copy()\n",
    "\n",
    "y_1 = train_1.visitors\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_1=train_1.drop(drop_cols, axis=1)\n",
    "test_1=test_1.drop(drop_cols, axis=1)\n",
    "\n",
    "y_2 = train_2.visitors\n",
    "\n",
    "train_2=train_2.drop(drop_cols, axis=1)\n",
    "test_2=test_2.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tes_316 = tes_316.drop([\"visitors\"],axis=1)\n",
    "_tes_513 = tes_513.drop([\"visitors\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define utility function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cross validation with LightGBM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, x_test, \n",
    "                        kf, \n",
    "                        cat_features=[],\n",
    "                        verbose=True, verbose_eval=100, nseeds=1, df_input=True,\n",
    "                        early_stopping=100, num_boost_round=8000, scoreonly=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "       \n",
    "    if len(cat_features)==0: use_cat=False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "        if df_input:\n",
    "            x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        else:\n",
    "            x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]\n",
    "\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "\n",
    "        for seed in range(nseeds):\n",
    "            params['feature_fraction_seed'] = seed\n",
    "            params['bagging_seed'] = seed\n",
    "\n",
    "            if use_cat:\n",
    "                lgb_train = lgb.Dataset(x_train_kf, y_train_kf, categorical_feature=cat_features)\n",
    "                lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train, categorical_feature=cat_features)\n",
    "\n",
    "            else:\n",
    "                lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n",
    "                lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n",
    "\n",
    "            gbm = lgb.train(params,\n",
    "                            lgb_train,\n",
    "                            num_boost_round=num_boost_round,\n",
    "                            valid_sets=[lgb_val],\n",
    "                            early_stopping_rounds=early_stopping,\n",
    "                            feval=feval_rmsle,\n",
    "                            verbose_eval=verbose_eval)\n",
    "\n",
    "            val_pred = np.expm1(gbm.predict(x_val_kf, num_iteration=gbm.best_iteration))\n",
    "            \n",
    "            train_pred[val_index] += val_pred\n",
    "            test_pred += np.expm1((gbm.predict(x_test, num_iteration=gbm.best_iteration)))\n",
    "\n",
    "                                \n",
    "        train_pred[val_index] = val_pred/nseeds\n",
    "\n",
    "        #fold_rmsle = rmsle(np.expm1(y_val_kf.values), train_pred[val_index])\n",
    "        fold_rmsle = rmsle(train_pred[val_index],np.expm1(y_val_kf.values))\n",
    "        if verbose:\n",
    "            print('fold cv {} RMSLE score is {:.6f}'.format(i, fold_rmsle))\n",
    "\n",
    "    test_pred = test_pred / (nseeds * kf.n_splits)\n",
    "    #cv_score = rmsle(y_train, train_pred)\n",
    "    cv_score = rmsle(train_pred, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_score))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    #return cv_score, np.expm1(train_pred),test_pred\n",
    "    \n",
    "    if scoreonly:\n",
    "        return cv_score\n",
    "    else:\n",
    "        return cv_score, train_pred, test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Modeling based on train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.570788\n",
      "[100]\tvalid_0's rmsle: 0.574997\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.503997\n",
      "fold cv 0 RMSLE score is 1.783158\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.577542\n",
      "[100]\tvalid_0's rmsle: 0.582236\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.506213\n",
      "fold cv 1 RMSLE score is 1.789501\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.573235\n",
      "[100]\tvalid_0's rmsle: 0.577793\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.503836\n",
      "fold cv 2 RMSLE score is 1.784790\n",
      "cv RMSLE score is 1.785819\n",
      "it takes 30.249 seconds to perform cross validation\n",
      "cv RMSLE score is 1.785819\n"
     ]
    }
   ],
   "source": [
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth' : 5,\n",
    "    'max_bin' : 500,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_leaves': 22,\n",
    "    #'metric': 'RMSE'\n",
    "}\n",
    "\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "#cv_score =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "#cv_score =cross_validate_lgb_nofeval(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "cv_score =cross_validate_lgb(lgb_params, train_1, y_1, test_1, kf, verbose=True, verbose_eval=50,df_input=True,scoreonly=True)\n",
    "\n",
    "print('cv RMSLE score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayesian Optimsation - Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'num_leaves':(7,4095),#(7,4095),\n",
    "    'max_depth':(2,63),\n",
    "    'learning_rate':(0.05,0.3),\n",
    "    'scale_pos_weight':(1,10000),\n",
    "    'min_sum_hessian_in_leaf':(2,30),\n",
    "    'subsample':(0.4,1.0),\n",
    "    'colsample_bytree':(0.4,1.0),\n",
    "    'feature_fraction':(0.1,0.9),\n",
    "    'bagging_fraction':(0.1,0.9),\n",
    "    'bagging_freq':(0,2),\n",
    "    'lambda_l1':(0.0,1.0),\n",
    "    'lambda_l2':(0.0,1.0),\n",
    "    'n_estimators':(2,30), \n",
    "    'reg_lambda':(0.0,2.0),\n",
    "    'min_gain_to_split':(0.0,1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(lgb_wrapper)\n",
    "#def lgbcv_func(max_depth, learning_rate, subsample, colsample_bytree, nthread=4, seed=0):\n",
    "def lgbcv_func(num_leaves, max_depth, learning_rate,\n",
    "               scale_pos_weight, \n",
    "               min_sum_hessian_in_leaf, \n",
    "               subsample, \n",
    "               colsample_bytree,\n",
    "               feature_fraction, bagging_fraction, \n",
    "               bagging_freq, lambda_l1, lambda_l2,\n",
    "               n_estimators,reg_lambda,min_gain_to_split,\n",
    "               nthread=4):\n",
    "\n",
    "    params = {\n",
    "        'objective' : \"regression\",\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "                \n",
    "        'num_leaves': int(num_leaves),\n",
    "        'max_depth': int(max_depth), \n",
    "        'learning_rate': float(learning_rate),\n",
    "        'scale_pos_weight':scale_pos_weight,\n",
    "        'min_sum_hessian_in_leaf':float(min_sum_hessian_in_leaf), \n",
    "        'subsample':subsample,\n",
    "        'colsample_bytree':colsample_bytree,\n",
    "        'feature_fraction':feature_fraction, \n",
    "        'bagging_fraction':bagging_fraction,\n",
    "        'bagging_freq':int(bagging_freq), \n",
    "        'lambda_l1':lambda_l1, \n",
    "        'lambda_l2':lambda_l2,\n",
    "        'n_estimators':n_estimators,\n",
    "        'reg_lambda':reg_lambda,\n",
    "        'min_gain_to_split':min_gain_to_split       \n",
    "        #'metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_lgb(params, train_1, y_1, test_1, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bo=BayesianOptimization(lgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   colsample_bytree |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_gain_to_split |   min_sum_hessian_in_leaf |   n_estimators |   num_leaves |   reg_lambda |   scale_pos_weight |   subsample | \n",
      "    1 | 08m19s | \u001b[35m  -0.76066\u001b[0m | \u001b[32m            0.5136\u001b[0m | \u001b[32m        0.6819\u001b[0m | \u001b[32m            0.8882\u001b[0m | \u001b[32m            0.6091\u001b[0m | \u001b[32m     0.4021\u001b[0m | \u001b[32m     0.1456\u001b[0m | \u001b[32m         0.0954\u001b[0m | \u001b[32m    55.9998\u001b[0m | \u001b[32m             0.1872\u001b[0m | \u001b[32m                   8.3007\u001b[0m | \u001b[32m       20.5923\u001b[0m | \u001b[32m   1871.9580\u001b[0m | \u001b[32m      0.4827\u001b[0m | \u001b[32m         9933.4246\u001b[0m | \u001b[32m     0.4414\u001b[0m | \n",
      "    2 | 05m37s | \u001b[35m  -0.71641\u001b[0m | \u001b[32m            0.1347\u001b[0m | \u001b[32m        0.6880\u001b[0m | \u001b[32m            0.9966\u001b[0m | \u001b[32m            0.7451\u001b[0m | \u001b[32m     0.6478\u001b[0m | \u001b[32m     0.3929\u001b[0m | \u001b[32m         0.1242\u001b[0m | \u001b[32m    59.0245\u001b[0m | \u001b[32m             0.3098\u001b[0m | \u001b[32m                  20.8397\u001b[0m | \u001b[32m       11.7705\u001b[0m | \u001b[32m   1946.2253\u001b[0m | \u001b[32m      0.0523\u001b[0m | \u001b[32m         5782.0772\u001b[0m | \u001b[32m     0.7074\u001b[0m | \n",
      "    3 | 06m26s | \u001b[35m   0.43971\u001b[0m | \u001b[32m            0.6409\u001b[0m | \u001b[32m        1.3783\u001b[0m | \u001b[32m            0.6957\u001b[0m | \u001b[32m            0.8778\u001b[0m | \u001b[32m     0.8337\u001b[0m | \u001b[32m     0.2951\u001b[0m | \u001b[32m         0.2743\u001b[0m | \u001b[32m    38.9031\u001b[0m | \u001b[32m             0.9944\u001b[0m | \u001b[32m                  13.4322\u001b[0m | \u001b[32m        3.9528\u001b[0m | \u001b[32m    472.5912\u001b[0m | \u001b[32m      1.4299\u001b[0m | \u001b[32m         7125.7741\u001b[0m | \u001b[32m     0.9019\u001b[0m | \n",
      "    4 | 03m25s |   -0.77038 |             0.8970 |         0.9146 |             0.9772 |             0.6899 |      0.9680 |      0.1366 |          0.1055 |     13.0106 |              0.0091 |                   29.0824 |        24.6899 |    3692.4273 |       1.7788 |          6227.4784 |      0.5611 | \n",
      "    5 | 01m54s |   -0.68284 |             0.8188 |         0.5569 |             0.9249 |             0.8350 |      0.4892 |      0.9102 |          0.1564 |     37.0508 |              0.8101 |                   19.5529 |        16.2087 |    3411.4744 |       0.0259 |          4237.5707 |      0.4644 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   colsample_bytree |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_gain_to_split |   min_sum_hessian_in_leaf |   n_estimators |   num_leaves |   reg_lambda |   scale_pos_weight |   subsample | \n",
      "    6 | 00m44s |   -0.81191 |             0.9000 |         2.0000 |             0.4000 |             0.1000 |      1.0000 |      0.0000 |          0.0500 |      2.0000 |              1.0000 |                    2.0000 |         2.0000 |       7.0000 |       2.0000 |             1.0000 |      1.0000 | \n",
      "    7 | 16m14s |    0.27182 |             0.1000 |         0.0000 |             1.0000 |             0.1000 |      0.0000 |      1.0000 |          0.3000 |     63.0000 |              0.0000 |                   30.0000 |        30.0000 |    4095.0000 |       0.0000 |             1.0000 |      0.4000 | \n",
      "    8 | 08m02s | \u001b[35m   0.48107\u001b[0m | \u001b[32m            0.9000\u001b[0m | \u001b[32m        2.0000\u001b[0m | \u001b[32m            0.4000\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \u001b[32m         0.3000\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m             1.0000\u001b[0m | \u001b[32m                   2.0000\u001b[0m | \u001b[32m       30.0000\u001b[0m | \u001b[32m      7.0000\u001b[0m | \u001b[32m      2.0000\u001b[0m | \u001b[32m         4052.7882\u001b[0m | \u001b[32m     0.4000\u001b[0m | \n",
      "    9 | 07m19s |    0.47864 |             0.9000 |         0.0000 |             0.4000 |             0.1000 |      0.0000 |      1.0000 |          0.3000 |      2.0000 |              0.0000 |                   30.0000 |        30.0000 |       7.0000 |       2.0000 |          8887.4891 |      0.4000 | \n",
      "   10 | 12m23s | \u001b[35m   0.48155\u001b[0m | \u001b[32m            0.9000\u001b[0m | \u001b[32m        0.0000\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \u001b[32m         0.3000\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m             0.0000\u001b[0m | \u001b[32m                  30.0000\u001b[0m | \u001b[32m       30.0000\u001b[0m | \u001b[32m      7.0000\u001b[0m | \u001b[32m      2.0000\u001b[0m | \u001b[32m         5779.5352\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "   11 | 01m09s |   -0.81191 |             0.1000 |         2.0000 |             0.4000 |             0.1000 |      0.0000 |      1.0000 |          0.0500 |      2.0000 |              0.0000 |                    2.0000 |         2.0000 |    4095.0000 |       2.0000 |          1705.4079 |      0.5571 | \n",
      "   12 | 01m36s |   -0.80010 |             0.1000 |         0.0000 |             1.0000 |             0.9000 |      0.0000 |      0.0000 |          0.0500 |     63.0000 |              0.0000 |                   30.0000 |        30.0000 |       7.0000 |       0.0000 |          2528.0902 |      0.4000 | \n",
      "   13 | 09m05s |    0.48016 |             0.9000 |         2.0000 |             0.4000 |             0.1000 |      1.0000 |      1.0000 |          0.3000 |      2.0000 |              1.0000 |                   30.0000 |         2.0000 |    4095.0000 |       2.0000 |         10000.0000 |      1.0000 | \n",
      "   14 | 23m00s |    0.26984 |             0.1000 |         0.0000 |             0.4000 |             0.1000 |      0.0000 |      1.0000 |          0.3000 |     63.0000 |              0.0000 |                    2.0000 |        30.0000 |    4095.0000 |       0.0000 |          8513.5145 |      0.6389 | \n",
      "   15 | 11m23s |    0.42617 |             0.9000 |         0.0000 |             1.0000 |             0.1000 |      1.0000 |      0.0000 |          0.3000 |     63.0000 |              1.0000 |                   30.0000 |        30.0000 |    2462.9024 |       0.0000 |             1.0000 |      0.4000 | \n",
      "   16 | 01m54s |   -0.80936 |             0.9000 |         2.0000 |             1.0000 |             0.1000 |      0.0000 |      1.0000 |          0.0500 |     63.0000 |              0.0000 |                    2.0000 |        30.0000 |       7.0000 |       2.0000 |          7802.4523 |      0.4000 | \n",
      "   17 | 03m43s |   -0.78627 |             0.3407 |         1.1417 |             0.4160 |             0.3008 |      0.5349 |      0.6603 |          0.0817 |     12.7340 |              0.1245 |                    2.1691 |         6.1153 |    3578.5994 |       1.7313 |          4434.3807 |      0.6643 | \n",
      "   18 | 17m42s |    0.47867 |             0.1000 |         0.0000 |             0.4000 |             0.1000 |      0.0000 |      1.0000 |          0.3000 |      2.0000 |              1.0000 |                   30.0000 |         2.0000 |       7.0000 |       2.0000 |         10000.0000 |      1.0000 | \n",
      "   19 | 16m09s | \u001b[35m   0.48408\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m        0.0000\u001b[0m | \u001b[32m            0.4000\u001b[0m | \u001b[32m            0.9000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \u001b[32m         0.3000\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m             1.0000\u001b[0m | \u001b[32m                  30.0000\u001b[0m | \u001b[32m        2.0000\u001b[0m | \u001b[32m   1812.0373\u001b[0m | \u001b[32m      0.0000\u001b[0m | \u001b[32m         7742.3536\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "   20 | 04m44s |   -0.80184 |             0.9000 |         0.8821 |             0.4000 |             0.1000 |      0.0000 |      1.0000 |          0.0500 |     63.0000 |              1.0000 |                   30.0000 |        30.0000 |    2016.0120 |       0.0000 |          1457.2429 |      0.4000 | \n"
     ]
    }
   ],
   "source": [
    "lgb_bo.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maximum value: 0.484076\n",
      "Best parameters:  {'num_leaves': 1812.0372756775496, 'max_depth': 2.0, 'learning_rate': 0.29999999999999999, 'scale_pos_weight': 7742.3535594133255, 'min_sum_hessian_in_leaf': 30.0, 'subsample': 1.0, 'colsample_bytree': 0.40000000000000002, 'feature_fraction': 0.90000000000000002, 'bagging_fraction': 0.10000000000000001, 'bagging_freq': 0.0, 'lambda_l1': 1.0, 'lambda_l2': 0.0, 'n_estimators': 2.0, 'reg_lambda': 0.0, 'min_gain_to_split': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % lgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', lgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Velification__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.499625\n",
      "fold cv 1 RMSLE score is 0.493109\n",
      "fold cv 2 RMSLE score is 0.494765\n",
      "fold cv 3 RMSLE score is 0.491680\n",
      "fold cv 4 RMSLE score is 0.494757\n",
      "fold cv 5 RMSLE score is 0.494452\n",
      "fold cv 6 RMSLE score is 0.495229\n",
      "fold cv 7 RMSLE score is 0.493174\n",
      "fold cv 8 RMSLE score is 0.489943\n",
      "fold cv 9 RMSLE score is 0.490895\n",
      "cv RMSLE score is 0.493775\n",
      "it takes 65.343 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "\n",
    "lgb_params = {\n",
    "    'num_leaves': int(1812.0372756775496), \n",
    "    'max_depth': int(2.0), \n",
    "    'learning_rate': 0.29999999999999999, \n",
    "    'scale_pos_weight': 7742.3535594133255, \n",
    "    'min_sum_hessian_in_leaf': 30.0, \n",
    "    'subsample': 1.0, \n",
    "    'colsample_bytree': 0.40000000000000002, \n",
    "    'feature_fraction': 0.90000000000000002, \n",
    "    'bagging_fraction': 0.10000000000000001, \n",
    "    'bagging_freq': int(0.0), \n",
    "    'lambda_l1': 1.0, \n",
    "    'lambda_l2': 0.0, \n",
    "    'n_estimators': 2.0, \n",
    "    'reg_lambda': 0.0, \n",
    "    'min_gain_to_split': 1.0\n",
    "} \n",
    "\n",
    "outcomes=cross_validate_lgb(lgb_params, train_1, y_1, test_1, kf, verbose_eval=False)\n",
    "\n",
    "lgb_cv=outcomes[0]\n",
    "lgb_train_1_pred=outcomes[1]\n",
    "lgb_test_1_pred=outcomes[2]\n",
    "\n",
    "lgb_train_1_pred_df=pd.DataFrame(columns=['visitors'], data=lgb_train_1_pred)\n",
    "lgb_test_1_pred_df=pd.DataFrame(columns=['visitors'], data=lgb_test_1_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling based on train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.567371\n",
      "[100]\tvalid_0's rmsle: 0.571482\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.498357\n",
      "fold cv 0 RMSLE score is 1.781551\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.569884\n",
      "[100]\tvalid_0's rmsle: 0.574181\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.499114\n",
      "fold cv 1 RMSLE score is 1.784154\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.564257\n",
      "[100]\tvalid_0's rmsle: 0.568736\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.49446\n",
      "fold cv 2 RMSLE score is 1.779858\n",
      "cv RMSLE score is 1.781856\n",
      "it takes 38.891 seconds to perform cross validation\n",
      "cv RMSLE score is 1.781856\n"
     ]
    }
   ],
   "source": [
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth' : 5,\n",
    "    'max_bin' : 500,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_leaves': 22,\n",
    "    #'metric': 'RMSE'\n",
    "}\n",
    "\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "#cv_score =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "#cv_score =cross_validate_lgb_nofeval(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "cv_score =cross_validate_lgb(lgb_params, train_2, y_2, test_2, kf, verbose=True, verbose_eval=50,df_input=True,scoreonly=True)\n",
    "\n",
    "print('cv RMSLE score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayesian Optimsation - Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'num_leaves':(7,4095),#(7,4095),\n",
    "    'max_depth':(2,63),\n",
    "    'learning_rate':(0.05,0.3),\n",
    "    'scale_pos_weight':(1,10000),\n",
    "    'min_sum_hessian_in_leaf':(2,30),\n",
    "    'subsample':(0.4,1.0),\n",
    "    'colsample_bytree':(0.4,1.0),\n",
    "    'feature_fraction':(0.1,0.9),\n",
    "    'bagging_fraction':(0.1,0.9),\n",
    "    'bagging_freq':(0,2),\n",
    "    'lambda_l1':(0.0,1.0),\n",
    "    'lambda_l2':(0.0,1.0),\n",
    "    'n_estimators':(2,30), \n",
    "    'reg_lambda':(0.0,2.0),\n",
    "    'min_gain_to_split':(0.0,1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(lgb_wrapper)\n",
    "#def lgbcv_func(max_depth, learning_rate, subsample, colsample_bytree, nthread=4, seed=0):\n",
    "def lgbcv_func(num_leaves, max_depth, learning_rate,\n",
    "               scale_pos_weight, \n",
    "               min_sum_hessian_in_leaf, \n",
    "               subsample, \n",
    "               colsample_bytree,\n",
    "               feature_fraction, bagging_fraction, \n",
    "               bagging_freq, lambda_l1, lambda_l2,\n",
    "               n_estimators,reg_lambda,min_gain_to_split,\n",
    "               nthread=4):\n",
    "\n",
    "    params = {\n",
    "        'objective' : \"regression\",\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "                \n",
    "        'num_leaves': int(num_leaves),\n",
    "        'max_depth': int(max_depth), \n",
    "        'learning_rate': float(learning_rate),\n",
    "        'scale_pos_weight':scale_pos_weight,\n",
    "        'min_sum_hessian_in_leaf':float(min_sum_hessian_in_leaf), \n",
    "        'subsample':subsample,\n",
    "        'colsample_bytree':colsample_bytree,\n",
    "        'feature_fraction':feature_fraction, \n",
    "        'bagging_fraction':bagging_fraction,\n",
    "        'bagging_freq':int(bagging_freq), \n",
    "        'lambda_l1':lambda_l1, \n",
    "        'lambda_l2':lambda_l2,\n",
    "        'n_estimators':n_estimators,\n",
    "        'reg_lambda':reg_lambda,\n",
    "        'min_gain_to_split':min_gain_to_split       \n",
    "        #'metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_lgb(params, train_2, y_2, test_2, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bo=BayesianOptimization(lgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   colsample_bytree |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_gain_to_split |   min_sum_hessian_in_leaf |   n_estimators |   num_leaves |   reg_lambda |   scale_pos_weight |   subsample | \n",
      "    1 | 02m04s | \u001b[35m  -0.78045\u001b[0m | \u001b[32m            0.5702\u001b[0m | \u001b[32m        1.7636\u001b[0m | \u001b[32m            0.7240\u001b[0m | \u001b[32m            0.3566\u001b[0m | \u001b[32m     0.6395\u001b[0m | \u001b[32m     0.7273\u001b[0m | \u001b[32m         0.1244\u001b[0m | \u001b[32m     5.0359\u001b[0m | \u001b[32m             0.7410\u001b[0m | \u001b[32m                   6.1572\u001b[0m | \u001b[32m       12.1459\u001b[0m | \u001b[32m   1279.2608\u001b[0m | \u001b[32m      0.8511\u001b[0m | \u001b[32m           81.7673\u001b[0m | \u001b[32m     0.9407\u001b[0m | \n",
      "    2 | 04m33s | \u001b[35m  -0.77453\u001b[0m | \u001b[32m            0.1739\u001b[0m | \u001b[32m        0.0193\u001b[0m | \u001b[32m            0.9980\u001b[0m | \u001b[32m            0.4265\u001b[0m | \u001b[32m     0.8235\u001b[0m | \u001b[32m     0.1244\u001b[0m | \u001b[32m         0.1893\u001b[0m | \u001b[32m    17.5587\u001b[0m | \u001b[32m             0.7979\u001b[0m | \u001b[32m                  18.3096\u001b[0m | \u001b[32m       13.5991\u001b[0m | \u001b[32m   3315.1630\u001b[0m | \u001b[32m      1.9887\u001b[0m | \u001b[32m         5691.1127\u001b[0m | \u001b[32m     0.4232\u001b[0m | \n",
      "    3 | 05m50s | \u001b[35m  -0.76455\u001b[0m | \u001b[32m            0.7553\u001b[0m | \u001b[32m        0.1960\u001b[0m | \u001b[32m            0.4523\u001b[0m | \u001b[32m            0.3666\u001b[0m | \u001b[32m     0.0395\u001b[0m | \u001b[32m     0.9753\u001b[0m | \u001b[32m         0.1661\u001b[0m | \u001b[32m    57.2956\u001b[0m | \u001b[32m             0.6690\u001b[0m | \u001b[32m                  28.0016\u001b[0m | \u001b[32m       12.9333\u001b[0m | \u001b[32m   2861.3797\u001b[0m | \u001b[32m      1.7055\u001b[0m | \u001b[32m         9531.1490\u001b[0m | \u001b[32m     0.4039\u001b[0m | \n",
      "    4 | 17m09s | \u001b[35m   0.45750\u001b[0m | \u001b[32m            0.8337\u001b[0m | \u001b[32m        0.9232\u001b[0m | \u001b[32m            0.5548\u001b[0m | \u001b[32m            0.7054\u001b[0m | \u001b[32m     0.3129\u001b[0m | \u001b[32m     0.6492\u001b[0m | \u001b[32m         0.2654\u001b[0m | \u001b[32m    55.9804\u001b[0m | \u001b[32m             0.7557\u001b[0m | \u001b[32m                  23.3046\u001b[0m | \u001b[32m       25.6361\u001b[0m | \u001b[32m   2972.2376\u001b[0m | \u001b[32m      1.0761\u001b[0m | \u001b[32m         3830.8304\u001b[0m | \u001b[32m     0.7810\u001b[0m | \n",
      "    5 | 06m05s |   -0.58316 |             0.5873 |         1.1303 |             0.7804 |             0.6866 |      0.1469 |      0.1706 |          0.2805 |     56.1616 |              0.0820 |                    3.9694 |        20.3614 |    1291.7163 |       1.5302 |          1530.1033 |      0.5410 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   colsample_bytree |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_gain_to_split |   min_sum_hessian_in_leaf |   n_estimators |   num_leaves |   reg_lambda |   scale_pos_weight |   subsample | \n",
      "    6 | 05m50s |   -0.79894 |             0.9000 |         0.0000 |             0.6150 |             0.1000 |      0.0000 |      0.0000 |          0.0500 |     63.0000 |              1.0000 |                   30.0000 |        30.0000 |    4095.0000 |       2.0000 |          1844.6859 |      1.0000 | \n",
      "    7 | 23m54s | \u001b[35m   0.50267\u001b[0m | \u001b[32m            0.9000\u001b[0m | \u001b[32m        2.0000\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m            0.9000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \u001b[32m         0.3000\u001b[0m | \u001b[32m    63.0000\u001b[0m | \u001b[32m             1.0000\u001b[0m | \u001b[32m                  30.0000\u001b[0m | \u001b[32m       30.0000\u001b[0m | \u001b[32m      7.0000\u001b[0m | \u001b[32m      0.0000\u001b[0m | \u001b[32m         4653.6550\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "    8 | 11m59s |    0.48356 |             0.1000 |         2.0000 |             0.4000 |             0.6387 |      0.0000 |      1.0000 |          0.3000 |      2.0000 |              0.0000 |                    2.0000 |         2.0000 |       7.0000 |       0.0000 |          7378.7723 |      1.0000 | \n",
      "    9 | 01m11s |   -0.80048 |             0.1000 |         2.0000 |             1.0000 |             0.9000 |      1.0000 |      0.0000 |          0.0500 |      2.0000 |              0.0000 |                    2.0000 |        30.0000 |       7.0000 |       0.0000 |         10000.0000 |      1.0000 | \n",
      "   10 | 01m34s |   -0.80935 |             0.9000 |         0.0000 |             0.4000 |             0.1000 |      0.0000 |      0.0000 |          0.0500 |      2.0000 |              1.0000 |                    2.0000 |         2.0000 |    1450.6319 |       2.0000 |          3987.7619 |      1.0000 | \n",
      "   11 | 09m04s |    0.19812 |             0.3621 |         1.8809 |             0.7781 |             0.8881 |      0.8756 |      0.3505 |          0.2634 |     51.3954 |              0.6137 |                   11.0390 |        13.6615 |      39.3098 |       0.1612 |          5192.6630 |      0.7613 | \n",
      "   12 | 22m15s |    0.36346 |             0.9000 |         0.0000 |             0.4000 |             0.1000 |      1.0000 |      0.0000 |          0.3000 |     63.0000 |              0.0000 |                   30.0000 |        30.0000 |    1454.7633 |       2.0000 |          7363.9735 |      0.4000 | \n",
      "   13 | 01m36s |   -0.80935 |             0.1000 |         0.0000 |             0.4000 |             0.1000 |      1.0000 |      0.0000 |          0.0500 |      2.0000 |              0.0000 |                    2.0000 |         2.0000 |    4095.0000 |       0.0000 |          3902.2972 |      0.4000 | \n",
      "   14 | 52m57s |    0.50223 |             0.9000 |         2.0000 |             1.0000 |             0.9000 |      1.0000 |      1.0000 |          0.3000 |      2.0000 |              0.0000 |                    2.0000 |        30.0000 |    4095.0000 |       0.0000 |          7773.3140 |      1.0000 | \n",
      "   15 | 01m18s |   -0.80055 |             0.1316 |         2.0000 |             0.4000 |             0.9000 |      0.0000 |      1.0000 |          0.0500 |      2.0000 |              1.0000 |                    2.0000 |         2.0000 |    2831.5070 |       0.0000 |          7587.2522 |      1.0000 | \n",
      "   16 | 01m34s |   -0.80935 |             0.9000 |         0.0000 |             1.0000 |             0.1000 |      0.0000 |      0.5145 |          0.0500 |      2.0000 |              1.0000 |                   30.0000 |        30.0000 |     789.5899 |       2.0000 |          6441.0899 |      0.4000 | \n",
      "   17 | 05m46s |   -0.58177 |             0.4831 |         1.1308 |             0.4000 |             0.5240 |      0.5829 |      0.0246 |          0.3000 |     45.3257 |              0.0000 |                    6.0583 |         7.9068 |     663.5437 |       0.3930 |          7860.3273 |      0.7437 | \n",
      "   18 | 05m40s |   -0.51227 |             0.5419 |         2.0000 |             1.0000 |             0.9000 |      1.0000 |      0.9024 |          0.3000 |     63.0000 |              0.1538 |                   30.0000 |        30.0000 |     495.8490 |       0.0000 |          4732.6004 |      0.5989 | \n",
      "   19 | 12m15s |   -0.77934 |             0.3731 |         0.5662 |             0.7383 |             0.7289 |      0.2996 |      0.3686 |          0.0826 |     46.7285 |              0.0749 |                    7.4095 |        25.9053 |    1292.6723 |       0.8515 |          1816.8159 |      0.8399 | \n",
      "   20 | 18m01s |    0.48876 |             0.1000 |         0.0000 |             1.0000 |             0.1000 |      0.0000 |      0.0000 |          0.3000 |      2.0000 |              0.0000 |                   30.0000 |         2.0000 |    4095.0000 |       2.0000 |             1.0000 |      1.0000 | \n"
     ]
    }
   ],
   "source": [
    "lgb_bo.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maximum value: 0.502675\n",
      "Best parameters:  {'num_leaves': 7.0, 'max_depth': 63.0, 'learning_rate': 0.29999999999999999, 'scale_pos_weight': 4653.6549687603901, 'min_sum_hessian_in_leaf': 30.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'feature_fraction': 0.90000000000000002, 'bagging_fraction': 0.90000000000000002, 'bagging_freq': 2.0, 'lambda_l1': 1.0, 'lambda_l2': 1.0, 'n_estimators': 30.0, 'reg_lambda': 0.0, 'min_gain_to_split': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % lgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', lgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Velification__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.488026\n",
      "fold cv 1 RMSLE score is 0.485111\n",
      "fold cv 2 RMSLE score is 0.480672\n",
      "fold cv 3 RMSLE score is 0.480342\n",
      "fold cv 4 RMSLE score is 0.472245\n",
      "fold cv 5 RMSLE score is 0.476952\n",
      "fold cv 6 RMSLE score is 0.479841\n",
      "fold cv 7 RMSLE score is 0.481549\n",
      "fold cv 8 RMSLE score is 0.473780\n",
      "fold cv 9 RMSLE score is 0.476972\n",
      "cv RMSLE score is 0.479580\n",
      "it takes 78.843 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "\n",
    "lgb_params = {\n",
    "    'num_leaves': int(7.0), \n",
    "    'max_depth': int(63.0), \n",
    "    'learning_rate': 0.29999999999999999, \n",
    "    'scale_pos_weight': 4653.6549687603901, \n",
    "    'min_sum_hessian_in_leaf': 30.0, \n",
    "    'subsample': 1.0, \n",
    "    'colsample_bytree': 1.0, \n",
    "    'feature_fraction': 0.90000000000000002, \n",
    "    'bagging_fraction': 0.90000000000000002, \n",
    "    'bagging_freq': int(2.0), \n",
    "    'lambda_l1': 1.0, \n",
    "    'lambda_l2': 1.0, \n",
    "    'n_estimators': 30.0, \n",
    "    'reg_lambda': 0.0, \n",
    "    'min_gain_to_split': 1.0} \n",
    "\n",
    "outcomes=cross_validate_lgb(lgb_params, train_2, y_2, test_2, kf, verbose_eval=False)\n",
    "\n",
    "lgb_cv=outcomes[0]\n",
    "lgb_train_2_pred=outcomes[1]\n",
    "lgb_test_2_pred=outcomes[2]\n",
    "\n",
    "lgb_train_2_pred_df=pd.DataFrame(columns=['visitors'], data=lgb_train_2_pred)\n",
    "lgb_test_2_pred_df=pd.DataFrame(columns=['visitors'], data=lgb_test_2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_lgb_train_1_pred = lgb_train_1_pred_df.copy()\n",
    "lv1_lgb_train_1_pred.to_csv('lv1_lgb_train_1_pred.csv', index=False)\n",
    "\n",
    "lv1_lgb_test_1_pred = lgb_test_1_pred_df.copy()\n",
    "lv1_lgb_test_1_pred.to_csv('lv1_lgb_test_1_pred.csv', index=False)\n",
    "\n",
    "_tes_316 = pd.concat([_tes_316,lv1_lgb_test_1_pred], axis=1)\n",
    "\n",
    "lv1_lgb_train_2_pred = lgb_train_2_pred_df.copy()\n",
    "lv1_lgb_train_2_pred.to_csv('lv1_lgb_train_2_pred.csv', index=False)\n",
    "\n",
    "lv1_lgb_test_2_pred = lgb_test_2_pred_df.copy()\n",
    "lv1_lgb_test_2_pred.to_csv('lv1_lgb_test_2_pred.csv', index=False)\n",
    "\n",
    "_tes_513 = pd.concat([_tes_513,lv1_lgb_test_2_pred], axis=1)\n",
    "\n",
    "lgb_test_pred_df = pd.concat([_tes_316,_tes_513])\n",
    "\n",
    "sub_ = lgb_test_pred_df[['id','visitors']].copy()\n",
    "\n",
    "sub_ = sub_.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "sub_.to_csv('submission_rs_recruit_v15_lgbm_fe_suzukiry_01.csv', index=False)\n",
    "print('Finished.')\n",
    "# lgbm\n",
    "# fe\n",
    "# bopt\n",
    "# train data separation\n",
    "# LB: ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Last work with weight__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v15_lgbm_fe_suzukiry_02.csv', index=False)\n",
    "# lgbm\n",
    "# fe\n",
    "# bopt\n",
    "# train data separation\n",
    "# weight\n",
    "# LB: ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
