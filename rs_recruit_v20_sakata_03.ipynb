{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sakata method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# OS\n",
    "import glob, re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# data science tool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# machine learning\n",
    "from sklearn import *\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# データの読み込み\n",
    "# 事前にcalendar_dateをvisit_dataに変更しています。airとhpgで同じことですが、別名で使用されているようです。\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "}\n",
    "\n",
    "# それぞれのデータをマージするために、まずは、relation用のものをマージします\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how = 'inner', on = ['hpg_store_id'])\n",
    "\n",
    "for df in ['ar', 'hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r:(r['visit_datetime']- r['reserve_datetime']).days, axis = 1)\n",
    "    tmp1 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how = 'inner', on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'dow':[i]*len(unique_stores)}) for i in range(7)], axis =0, ignore_index = True).reset_index(drop = True) \n",
    "\n",
    "#曜日だけでなく、月も追加\n",
    "stores_m = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'month':[i]*len(unique_stores)}) for i in range(1,13)], axis =0, ignore_index = True).reset_index(drop = True)\n",
    "stores = pd.merge(stores_m, stores,on=('air_store_id'), how='left')\n",
    "\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].min().rename(columns = {'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].median().rename(columns = {'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].max().rename(columns = {'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].count().rename(columns = {'visitors':'count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "\n",
    "#曜日だけでなく、ID×月も追加\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].min().rename(columns = {'visitors':'m_min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'m_mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].median().rename(columns = {'visitors':'m_median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].max().rename(columns = {'visitors':'m_max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].count().rename(columns = {'visitors':'m_count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "\n",
    "stores = pd.merge(stores, data['as'], how= \"left\", on = ['air_store_id'])\n",
    "\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/', ' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-', ' ')))\n",
    "\n",
    "lbl_gn = preprocessing.LabelEncoder()\n",
    "lbl_an = preprocessing.LabelEncoder()\n",
    "\n",
    "for i in range(10):\n",
    "    stores['air_genre_name' + str(i)] = lbl_gn.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "    stores['air_area_name' + str(i)] = lbl_an.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "stores['air_genre_name'] = lbl_gn.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl_an.fit_transform(stores['air_area_name'])\n",
    "\n",
    "\n",
    "#lbl = preprocessing.LabelEncoder()\n",
    "#for i in range(10):\n",
    "#    stores['air_genre_name' + str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "#    stores['air_area_name' + str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "#stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "#stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "\n",
    "#土日フラグ(day_of_week_1)と、休日前(holi_2)フラグを追加\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "data['hol']['day_of_week_1']= data['hol']['day_of_week'].replace(['Saturday', 'Sunday','Monday','Tuesday','Wednesday','Thursday','Friday'],['1', '1','0','0','0','0','0']).astype('int')\n",
    "data['hol']['holi_2'] = data['hol'][['holiday_flg', 'day_of_week_1']].sum(axis = 1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].apply( lambda x: 0 if x < 1 else 1 )\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].shift(-1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].fillna(1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].astype('int')\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how ='left', on = ['visit_date'])\n",
    "test = pd.merge(data['tes'], data['hol'], how ='left', on = ['visit_date'])\n",
    "\n",
    "#曜日と月でmerge\n",
    "train = pd.merge(train, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "test = pd.merge(test, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "\n",
    "#ID×休日前でのvisitorsの平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].min().rename(columns = {'visitors':'h_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'h_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].median().rename(columns = {'visitors':'h_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].max().rename(columns = {'visitors':'h_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].count().rename(columns = {'visitors':'h_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1) \n",
    "\n",
    "train['total_reserve_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserve_mean'] = (train['rv2_x'] + train['rv2_y'])/2\n",
    "train['total_reserve_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y'])/2\n",
    "\n",
    "test['total_reserve_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserve_mean'] = (test['rv2_x'] + test['rv2_y'])/2\n",
    "test['total_reserve_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y'])/2\n",
    "\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2']= lbl.fit_transform(test['air_store_id'])\n",
    "\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date', 'visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "\n",
    "all_data = pd.concat([train, test]) \n",
    "\n",
    "#指数移動平均の追加。これはなくても良いかも\n",
    "#https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46179#266344\n",
    "#def calc_shifted_ewm(series, alpha, adjust=True):\n",
    "#    return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n",
    "\n",
    "#train['ewm'] = train.groupby(['air_store_id', 'dow']).apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1)).sort_index(level=['air_store_id']).values\n",
    "\n",
    "#以下、気象データの追加\n",
    "df_air_store_weather_station = pd.read_csv('../../../mltestdata/05_recruit/air_store_info_with_nearest_active_station.csv')\n",
    "\n",
    "cols = ['air_store_id', 'station_id', 'station_latitude', 'station_longitude', 'station_vincenty', 'station_great_circle']\n",
    "all_data = pd.merge(all_data, df_air_store_weather_station[cols], on='air_store_id', how='left')\n",
    "\n",
    "combine = all_data\n",
    "filenames = []\n",
    "df_weather = None\n",
    "for station_id in combine['station_id'].unique():\n",
    "    fn = f\"../../../mltestdata/05_recruit/1-1-16_5-31-17_Weather/{station_id}.csv\"\n",
    "    if not fn in filenames:\n",
    "        df = pd.read_csv(fn)\n",
    "        df['station_id'] = station_id\n",
    "        if df_weather is None:\n",
    "            df_weather = df\n",
    "        else:\n",
    "            df_weather = pd.concat([df_weather, df])\n",
    "        del df\n",
    "\n",
    "        filenames.append(fn)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#欠損値を平均で穴埋め（median, ffillで試すも特に差は出なかった）\n",
    "df_weather = df_weather.fillna(df_weather.mean())\n",
    "\n",
    "df_weather = df_weather.rename(columns={'calendar_date': 'visit_date'})\n",
    "\n",
    "df_weather['visit_date'] = pd.to_datetime(df_weather['visit_date'])\n",
    "df_weather['visit_date'] = df_weather['visit_date'].dt.date\n",
    "\n",
    "#なんとなく対数化\n",
    "df_weather['precipitation'] = np.log1p(df_weather['precipitation'])\n",
    "\n",
    "#使いそうなデータだけ結合（その他の気象データは試していません。特に意味はなし）\n",
    "cols = ['station_id', \n",
    "    'visit_date', \n",
    "    'precipitation', \n",
    "    'hours_sunlight',\n",
    "    'avg_temperature',\n",
    "    'high_temperature',\n",
    "    'low_temperature']\n",
    "\n",
    "combine = pd.merge(combine, df_weather[cols], on=['station_id', 'visit_date'], how='left')\n",
    "\n",
    "#降水量をカテゴリ化\n",
    "def simplify_pre(df):\n",
    "    df.precipitation = df.precipitation.fillna(0)\n",
    "    bins = ( -1, 0.01, 2,  5)\n",
    "    group_names = ['1', '2', '3']\n",
    "    categories = pd.cut(df.precipitation, bins, labels=group_names)\n",
    "    df.precipitation = categories\n",
    "    return df\n",
    "\n",
    "combine = simplify_pre(combine) \n",
    "all_data = combine \n",
    "\n",
    "#不要そうなデータを削除\n",
    "drop_col =['station_id', 'station_latitude','station_longitude','station_vincenty', 'station_great_circle','hours_sunlight','high_temperature','low_temperature']\n",
    "all_data = all_data.drop(drop_col, axis = 1)\n",
    "\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "\n",
    "#ID×降水量で平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].min().rename(columns = {'visitors':'p_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'p_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].median().rename(columns = {'visitors':'p_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation',])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].max().rename(columns = {'visitors':'p_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].count().rename(columns = {'visitors':'p_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "#countをLabel Encoder化。なんとなく試してみたら結果が良かった。\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['count_visitors'] = lbl.fit_transform(train['count_visitors']) \n",
    "test['count_visitors']= lbl.fit_transform(test['count_visitors'])\n",
    "train['m_count_visitors'] = lbl.fit_transform(train['m_count_visitors'])\n",
    "test['m_count_visitors']= lbl.fit_transform(test['m_count_visitors'])\n",
    "train['h_count_visitors'] = lbl.fit_transform(train['h_count_visitors'])\n",
    "test['h_count_visitors']= lbl.fit_transform(test['h_count_visitors'])\n",
    "train['p_count_visitors'] = lbl.fit_transform(train['p_count_visitors'])\n",
    "test['p_count_visitors']= lbl.fit_transform(test['p_count_visitors'])\n",
    "\n",
    "# GW flag\n",
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]\n",
    "\n",
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precipitation'] = train['precipitation'].values.astype(float)\n",
    "test['precipitation'] = test['precipitation'].values.astype(float)\n",
    "\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252108, 75)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 75)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refered to this [super cool eda](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/45048). I will categorize as below.\n",
    "- Izakaya\n",
    "- Cafe Sweets\n",
    "- others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Asian', 'Bar Cocktail', 'Cafe Sweets', 'Creative cuisine',\n",
       "       'Dining bar', 'International cuisine', 'Italian French', 'Izakaya',\n",
       "       'Japanese food', 'Karaoke Party', 'Okonomiyaki Monja Teppanyaki',\n",
       "       'Other', 'Western food', 'Yakiniku Korean food'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_gn.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0     7605\n",
       "2.0     6903\n",
       "4.0     4173\n",
       "6.0     3939\n",
       "1.0     3081\n",
       "8.0     2457\n",
       "11.0    1053\n",
       "13.0     897\n",
       "12.0     624\n",
       "10.0     546\n",
       "3.0      507\n",
       "5.0       78\n",
       "9.0       78\n",
       "0.0       78\n",
       "Name: air_genre_name, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.air_genre_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Izakaya: [7]\n",
      "Cafe Sweets: [2]\n",
      "Italian French: [6]\n",
      "Dining bar: [4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Izakaya: \"+str(lbl_gn.transform(['Izakaya'])))\n",
    "print(\"Cafe Sweets: \"+str(lbl_gn.transform(['Cafe Sweets'])))\n",
    "print(\"Italian French: \"+str(lbl_gn.transform(['Italian French'])))\n",
    "print(\"Dining bar: \"+str(lbl_gn.transform(['Dining bar'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_iza = train[train.air_genre_name == 7]\n",
    "x_train_caf = train[train.air_genre_name == 2]\n",
    "x_train_ita = train[(train.air_genre_name == 4) | (train.air_genre_name == 6)]\n",
    "x_train_oth = train[(train.air_genre_name != 7) & (train.air_genre_name != 2) & (train.air_genre_name != 4) & (train.air_genre_name != 6)]\n",
    "\n",
    "y_train_iza=x_train_iza.visitors\n",
    "y_train_caf=x_train_caf.visitors\n",
    "y_train_ita=x_train_ita.visitors\n",
    "y_train_oth=x_train_oth.visitors\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "x_train_iza=x_train_iza.drop(drop_cols, axis=1)\n",
    "x_train_caf=x_train_caf.drop(drop_cols, axis=1)\n",
    "x_train_ita=x_train_ita.drop(drop_cols, axis=1)\n",
    "x_train_oth=x_train_oth.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test_iza = test[test.air_genre_name == 7]\n",
    "x_test_caf = test[test.air_genre_name == 2]\n",
    "x_test_ita = test[(test.air_genre_name == 4) | (test.air_genre_name == 6)]\n",
    "x_test_oth = test[(test.air_genre_name != 7) & (test.air_genre_name != 2) & (test.air_genre_name != 4) & (test.air_genre_name != 6)]\n",
    "\n",
    "x_test_iza=x_test_iza.drop(drop_cols, axis=1)\n",
    "x_test_caf=x_test_caf.drop(drop_cols, axis=1)\n",
    "x_test_ita=x_test_ita.drop(drop_cols, axis=1)\n",
    "x_test_oth=x_test_oth.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hyōgo ken Nishinomiya shi Rokutanjichō'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_an.inverse_transform(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dining bar'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_gn.inverse_transform(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tōkyō to Taitō ku Higashiueno'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_an.inverse_transform(82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gn: 175    2.0\n",
      "Name: air_genre_name, dtype: float64\n",
      "an: 175    102.0\n",
      "Name: air_area_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"gn: \"+str(test[175:176].air_genre_name))\n",
    "print(\"an: \"+str(test[175:176].air_area_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gn: 225    2.0\n",
      "Name: air_genre_name, dtype: float64\n",
      "an: 225    31.0\n",
      "Name: air_area_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"gn: \"+str(test[225:226].air_genre_name))\n",
    "print(\"an: \"+str(test[225:226].air_area_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gn: 575    2.0\n",
      "Name: air_genre_name, dtype: float64\n",
      "an: 575    30.0\n",
      "Name: air_area_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"gn: \"+str(test[575:576].air_genre_name))\n",
    "print(\"an: \"+str(test[575:576].air_area_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    #rmsle = np.sqrt(mean_squared_error(np.float64(np.log1p(true)), np.float64(np.log1p(preds))))\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)\n",
    "\n",
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)\n",
    "\n",
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, kf,  verbose=True, verbose_eval=50, scoreonly=False):\n",
    "    start_time=time.time()\n",
    "    nround=[]\n",
    "    # the prediction matrix need to contains 3 columns, one for the probability of each class\n",
    "    #train_pred = np.zeros((x_train.shape[0],3))\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    \n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "        #return 'rmsle', rmsle(true, preds), False\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "\n",
    "        #y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "        x_test_kf=x_test.copy()\n",
    "        \n",
    "        d_train = xgboost.DMatrix(x_train_kf, y_train_kf)\n",
    "        d_val=xgboost.DMatrix(x_val_kf, y_val_kf)\n",
    "        d_test = xgboost.DMatrix(x_test_kf)\n",
    "        \n",
    "        watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "        bst = xgboost.train(params=params, \n",
    "                            dtrain=d_train, \n",
    "                            num_boost_round=8000, \n",
    "                            early_stopping_rounds=100,\n",
    "                            evals=watchlist, \n",
    "                            verbose_eval=verbose_eval)        \n",
    "        \n",
    "#        y_val_kf_preds=bst.predict(d_val, ntree_limit=bst.best_ntree_limit)\n",
    "        y_val_kf_preds=np.expm1(bst.predict(d_val, ntree_limit=bst.best_ntree_limit))\n",
    "        nround.append(bst.best_ntree_limit)\n",
    "        \n",
    "        train_pred[val_index] += y_val_kf_preds\n",
    "#        test_pred += np.expm1((bst.predict(x_test, ntree_limit=bst.best_ntree_limit)))\n",
    "        test_pred += np.expm1(bst.predict(d_test, ntree_limit=bst.best_ntree_limit))\n",
    "        \n",
    "        \n",
    "        #fold_cv = log_loss(y_val_kf.values, y_val_kf_preds)\n",
    "        #fold_rmsle = rmsle(np.expm1(train_pred[val_index]),np.expm1(y_val_kf.values))\n",
    "        fold_rmsle = rmsle(train_pred[val_index],np.expm1(y_val_kf.values))\n",
    "        fold_cv = fold_rmsle\n",
    "        \n",
    "        if verbose:\n",
    "            print('fold cv {} rmsle score is {:.6f}'.format(i, fold_cv))\n",
    "\n",
    "    test_pred = test_pred / kf.n_splits\n",
    "    #cv_score = log_loss(y_train, train_pred)\n",
    "    #cv_score = rmsle(np.expm1(train_pred), y_train)\n",
    "    cv_score = rmsle(train_pred, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv rmsle score is {:.6f}'.format(cv_score))    \n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    " \n",
    "    if scoreonly:\n",
    "        #return cv_score # for the purpose of bayesian optimisation, we only need to return the CV score\n",
    "        return cv_score\n",
    "    else:\n",
    "        return (cv_score,train_pred,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#行数\n",
    "pd.set_option(\"display.max_rows\", 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train for Izakaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.02,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.49281\tval-rmse:2.49244\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:1.02485\tval-rmse:1.02664\n",
      "[100]\ttrain-rmse:0.593777\tval-rmse:0.599106\n",
      "[150]\ttrain-rmse:0.502984\tval-rmse:0.51084\n",
      "[200]\ttrain-rmse:0.484967\tval-rmse:0.494752\n",
      "[250]\ttrain-rmse:0.479075\tval-rmse:0.49031\n",
      "[300]\ttrain-rmse:0.47594\tval-rmse:0.488404\n",
      "[350]\ttrain-rmse:0.47359\tval-rmse:0.486983\n",
      "[400]\ttrain-rmse:0.471986\tval-rmse:0.486132\n",
      "[450]\ttrain-rmse:0.470334\tval-rmse:0.485406\n",
      "[500]\ttrain-rmse:0.469209\tval-rmse:0.484943\n",
      "[550]\ttrain-rmse:0.467943\tval-rmse:0.484308\n",
      "[600]\ttrain-rmse:0.467065\tval-rmse:0.483952\n",
      "[650]\ttrain-rmse:0.466117\tval-rmse:0.483603\n",
      "[700]\ttrain-rmse:0.465404\tval-rmse:0.483342\n",
      "[750]\ttrain-rmse:0.464634\tval-rmse:0.483133\n",
      "[800]\ttrain-rmse:0.463568\tval-rmse:0.482841\n",
      "[850]\ttrain-rmse:0.462793\tval-rmse:0.48257\n",
      "[900]\ttrain-rmse:0.462129\tval-rmse:0.482346\n",
      "[950]\ttrain-rmse:0.461261\tval-rmse:0.482001\n",
      "[1000]\ttrain-rmse:0.460502\tval-rmse:0.481767\n",
      "[1050]\ttrain-rmse:0.459676\tval-rmse:0.481499\n",
      "[1100]\ttrain-rmse:0.459161\tval-rmse:0.481342\n",
      "[1150]\ttrain-rmse:0.458726\tval-rmse:0.481152\n",
      "[1200]\ttrain-rmse:0.458003\tval-rmse:0.480967\n",
      "[1250]\ttrain-rmse:0.457234\tval-rmse:0.480762\n",
      "[1300]\ttrain-rmse:0.456616\tval-rmse:0.480574\n",
      "[1350]\ttrain-rmse:0.456052\tval-rmse:0.480429\n",
      "[1400]\ttrain-rmse:0.455607\tval-rmse:0.480298\n",
      "[1450]\ttrain-rmse:0.455181\tval-rmse:0.480201\n",
      "[1500]\ttrain-rmse:0.454344\tval-rmse:0.480002\n",
      "[1550]\ttrain-rmse:0.453712\tval-rmse:0.479862\n",
      "[1600]\ttrain-rmse:0.453094\tval-rmse:0.479732\n",
      "[1650]\ttrain-rmse:0.452571\tval-rmse:0.479651\n",
      "[1700]\ttrain-rmse:0.451858\tval-rmse:0.479513\n",
      "[1750]\ttrain-rmse:0.451412\tval-rmse:0.47944\n",
      "[1800]\ttrain-rmse:0.450819\tval-rmse:0.479397\n",
      "[1850]\ttrain-rmse:0.450269\tval-rmse:0.47925\n",
      "[1900]\ttrain-rmse:0.44959\tval-rmse:0.479151\n",
      "[1950]\ttrain-rmse:0.449128\tval-rmse:0.479135\n",
      "[2000]\ttrain-rmse:0.448629\tval-rmse:0.478979\n",
      "[2050]\ttrain-rmse:0.448201\tval-rmse:0.478975\n",
      "[2100]\ttrain-rmse:0.447811\tval-rmse:0.478922\n",
      "[2150]\ttrain-rmse:0.447253\tval-rmse:0.478883\n",
      "[2200]\ttrain-rmse:0.446797\tval-rmse:0.478826\n",
      "[2250]\ttrain-rmse:0.446436\tval-rmse:0.478797\n",
      "[2300]\ttrain-rmse:0.446091\tval-rmse:0.478779\n",
      "[2350]\ttrain-rmse:0.445612\tval-rmse:0.478738\n",
      "[2400]\ttrain-rmse:0.445105\tval-rmse:0.478637\n",
      "[2450]\ttrain-rmse:0.444699\tval-rmse:0.478509\n",
      "[2500]\ttrain-rmse:0.444257\tval-rmse:0.478474\n",
      "[2550]\ttrain-rmse:0.443926\tval-rmse:0.478458\n",
      "[2600]\ttrain-rmse:0.443605\tval-rmse:0.478454\n",
      "[2650]\ttrain-rmse:0.4432\tval-rmse:0.478416\n",
      "[2700]\ttrain-rmse:0.442815\tval-rmse:0.478349\n",
      "[2750]\ttrain-rmse:0.442472\tval-rmse:0.478384\n",
      "[2800]\ttrain-rmse:0.441998\tval-rmse:0.478378\n",
      "Stopping. Best iteration:\n",
      "[2708]\ttrain-rmse:0.442765\tval-rmse:0.478342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train_iza,y_train_iza,test_size=0.2,random_state=2018)\n",
    "\n",
    "y_train_log, y_test_log = np.log1p(y_train), np.log1p(y_test)\n",
    "\n",
    "d_train = xgboost.DMatrix(X_train, y_train_log)\n",
    "d_val=xgboost.DMatrix(X_test, y_test_log)\n",
    "d_test = xgboost.DMatrix(x_test_iza)\n",
    "\n",
    "watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "bst = xgboost.train(params=xgb_params, \n",
    "                    dtrain=d_train, \n",
    "                    num_boost_round=8000, \n",
    "                    early_stopping_rounds=100,\n",
    "                    evals=watchlist, \n",
    "                    verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_iza_log = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "y_pred_iza = np.expm1(y_pred_iza_log)\n",
    "\n",
    "x_test_iza[\"visitors\"] = y_pred_iza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7605, 72)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_iza.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train for Cafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.47053\tval-rmse:2.4729\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.992627\tval-rmse:0.994421\n",
      "[100]\ttrain-rmse:0.542249\tval-rmse:0.547989\n",
      "[150]\ttrain-rmse:0.442016\tval-rmse:0.451772\n",
      "[200]\ttrain-rmse:0.421323\tval-rmse:0.433806\n",
      "[250]\ttrain-rmse:0.413937\tval-rmse:0.428431\n",
      "[300]\ttrain-rmse:0.409562\tval-rmse:0.425805\n",
      "[350]\ttrain-rmse:0.406607\tval-rmse:0.424299\n",
      "[400]\ttrain-rmse:0.404147\tval-rmse:0.423103\n",
      "[450]\ttrain-rmse:0.402066\tval-rmse:0.422068\n",
      "[500]\ttrain-rmse:0.400276\tval-rmse:0.421496\n",
      "[550]\ttrain-rmse:0.398358\tval-rmse:0.420927\n",
      "[600]\ttrain-rmse:0.396683\tval-rmse:0.420302\n",
      "[650]\ttrain-rmse:0.395214\tval-rmse:0.419845\n",
      "[700]\ttrain-rmse:0.393672\tval-rmse:0.419402\n",
      "[750]\ttrain-rmse:0.392237\tval-rmse:0.419148\n",
      "[800]\ttrain-rmse:0.391072\tval-rmse:0.418912\n",
      "[850]\ttrain-rmse:0.389834\tval-rmse:0.418532\n",
      "[900]\ttrain-rmse:0.388673\tval-rmse:0.418251\n",
      "[950]\ttrain-rmse:0.387483\tval-rmse:0.418044\n",
      "[1000]\ttrain-rmse:0.386362\tval-rmse:0.41772\n",
      "[1050]\ttrain-rmse:0.385269\tval-rmse:0.417572\n",
      "[1100]\ttrain-rmse:0.384296\tval-rmse:0.417511\n",
      "[1150]\ttrain-rmse:0.383211\tval-rmse:0.4174\n",
      "[1200]\ttrain-rmse:0.382048\tval-rmse:0.41722\n",
      "[1250]\ttrain-rmse:0.381013\tval-rmse:0.417129\n",
      "[1300]\ttrain-rmse:0.380085\tval-rmse:0.417028\n",
      "[1350]\ttrain-rmse:0.379122\tval-rmse:0.41696\n",
      "[1400]\ttrain-rmse:0.378123\tval-rmse:0.416816\n",
      "[1450]\ttrain-rmse:0.377247\tval-rmse:0.41674\n",
      "[1500]\ttrain-rmse:0.376358\tval-rmse:0.416631\n",
      "[1550]\ttrain-rmse:0.375609\tval-rmse:0.416589\n",
      "[1600]\ttrain-rmse:0.374774\tval-rmse:0.41648\n",
      "[1650]\ttrain-rmse:0.373934\tval-rmse:0.416424\n",
      "[1700]\ttrain-rmse:0.373142\tval-rmse:0.416401\n",
      "[1750]\ttrain-rmse:0.372394\tval-rmse:0.416423\n",
      "[1800]\ttrain-rmse:0.371553\tval-rmse:0.416346\n",
      "[1850]\ttrain-rmse:0.370705\tval-rmse:0.416287\n",
      "[1900]\ttrain-rmse:0.369914\tval-rmse:0.416245\n",
      "[1950]\ttrain-rmse:0.369179\tval-rmse:0.416252\n",
      "[2000]\ttrain-rmse:0.368539\tval-rmse:0.416253\n",
      "[2050]\ttrain-rmse:0.367925\tval-rmse:0.416214\n",
      "[2100]\ttrain-rmse:0.367244\tval-rmse:0.416156\n",
      "[2150]\ttrain-rmse:0.36658\tval-rmse:0.416178\n",
      "[2200]\ttrain-rmse:0.36586\tval-rmse:0.416241\n",
      "[2250]\ttrain-rmse:0.365122\tval-rmse:0.416256\n",
      "Stopping. Best iteration:\n",
      "[2153]\ttrain-rmse:0.366549\tval-rmse:0.416152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.02,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_caf,y_train_caf,test_size=0.2,random_state=2018)\n",
    "\n",
    "y_train_log, y_test_log = np.log1p(y_train), np.log1p(y_test)\n",
    "\n",
    "d_train = xgboost.DMatrix(X_train, y_train_log)\n",
    "d_val=xgboost.DMatrix(X_test, y_test_log)\n",
    "d_test = xgboost.DMatrix(x_test_caf)\n",
    "\n",
    "watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "bst = xgboost.train(params=xgb_params, \n",
    "                    dtrain=d_train, \n",
    "                    num_boost_round=8000, \n",
    "                    early_stopping_rounds=100,\n",
    "                    evals=watchlist, \n",
    "                    verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6903, 72)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_caf_log = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "y_pred_caf = np.expm1(y_pred_caf_log)\n",
    "\n",
    "x_test_caf[\"visitors\"] = y_pred_caf\n",
    "\n",
    "x_test_caf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train for Italian & French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.36569\tval-rmse:2.3682\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.993979\tval-rmse:0.98983\n",
      "[100]\ttrain-rmse:0.603653\tval-rmse:0.596722\n",
      "[150]\ttrain-rmse:0.523668\tval-rmse:0.51776\n",
      "[200]\ttrain-rmse:0.506887\tval-rmse:0.502886\n",
      "[250]\ttrain-rmse:0.501048\tval-rmse:0.498777\n",
      "[300]\ttrain-rmse:0.497348\tval-rmse:0.496777\n",
      "[350]\ttrain-rmse:0.494332\tval-rmse:0.495456\n",
      "[400]\ttrain-rmse:0.491726\tval-rmse:0.494505\n",
      "[450]\ttrain-rmse:0.489395\tval-rmse:0.493702\n",
      "[500]\ttrain-rmse:0.487458\tval-rmse:0.493179\n",
      "[550]\ttrain-rmse:0.485672\tval-rmse:0.492664\n",
      "[600]\ttrain-rmse:0.48399\tval-rmse:0.492245\n",
      "[650]\ttrain-rmse:0.482571\tval-rmse:0.491815\n",
      "[700]\ttrain-rmse:0.481295\tval-rmse:0.491583\n",
      "[750]\ttrain-rmse:0.480189\tval-rmse:0.491392\n",
      "[800]\ttrain-rmse:0.479032\tval-rmse:0.491202\n",
      "[850]\ttrain-rmse:0.478174\tval-rmse:0.491134\n",
      "[900]\ttrain-rmse:0.477388\tval-rmse:0.490915\n",
      "[950]\ttrain-rmse:0.476701\tval-rmse:0.490851\n",
      "[1000]\ttrain-rmse:0.476056\tval-rmse:0.490804\n",
      "[1050]\ttrain-rmse:0.475201\tval-rmse:0.490676\n",
      "[1100]\ttrain-rmse:0.474372\tval-rmse:0.490596\n",
      "[1150]\ttrain-rmse:0.473661\tval-rmse:0.490478\n",
      "[1200]\ttrain-rmse:0.473246\tval-rmse:0.490393\n",
      "[1250]\ttrain-rmse:0.472506\tval-rmse:0.490371\n",
      "[1300]\ttrain-rmse:0.471849\tval-rmse:0.490267\n",
      "[1350]\ttrain-rmse:0.4713\tval-rmse:0.490283\n",
      "[1400]\ttrain-rmse:0.470799\tval-rmse:0.490283\n",
      "Stopping. Best iteration:\n",
      "[1303]\ttrain-rmse:0.471792\tval-rmse:0.490264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.02,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_ita,y_train_ita,test_size=0.2,random_state=2018)\n",
    "\n",
    "y_train_log, y_test_log = np.log1p(y_train), np.log1p(y_test)\n",
    "\n",
    "d_train = xgboost.DMatrix(X_train, y_train_log)\n",
    "d_val=xgboost.DMatrix(X_test, y_test_log)\n",
    "d_test = xgboost.DMatrix(x_test_ita)\n",
    "\n",
    "watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "bst = xgboost.train(params=xgb_params, \n",
    "                    dtrain=d_train, \n",
    "                    num_boost_round=8000, \n",
    "                    early_stopping_rounds=100,\n",
    "                    evals=watchlist, \n",
    "                    verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8112, 72)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ita_log = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "y_pred_ita = np.expm1(y_pred_ita_log)\n",
    "\n",
    "x_test_ita[\"visitors\"] = y_pred_ita\n",
    "\n",
    "x_test_ita.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train for Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.27565\tval-rmse:2.27948\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.959318\tval-rmse:0.960167\n",
      "[100]\ttrain-rmse:0.587249\tval-rmse:0.58569\n",
      "[150]\ttrain-rmse:0.512069\tval-rmse:0.51056\n",
      "[200]\ttrain-rmse:0.496622\tval-rmse:0.496297\n",
      "[250]\ttrain-rmse:0.491056\tval-rmse:0.492069\n",
      "[300]\ttrain-rmse:0.487598\tval-rmse:0.490008\n",
      "[350]\ttrain-rmse:0.484632\tval-rmse:0.488419\n",
      "[400]\ttrain-rmse:0.482148\tval-rmse:0.487237\n",
      "[450]\ttrain-rmse:0.480129\tval-rmse:0.486316\n",
      "[500]\ttrain-rmse:0.478165\tval-rmse:0.485526\n",
      "[550]\ttrain-rmse:0.476502\tval-rmse:0.48489\n",
      "[600]\ttrain-rmse:0.474814\tval-rmse:0.484287\n",
      "[650]\ttrain-rmse:0.473332\tval-rmse:0.483867\n",
      "[700]\ttrain-rmse:0.471843\tval-rmse:0.483454\n",
      "[750]\ttrain-rmse:0.470421\tval-rmse:0.483026\n",
      "[800]\ttrain-rmse:0.469004\tval-rmse:0.48267\n",
      "[850]\ttrain-rmse:0.467604\tval-rmse:0.482332\n",
      "[900]\ttrain-rmse:0.466241\tval-rmse:0.481905\n",
      "[950]\ttrain-rmse:0.464891\tval-rmse:0.481504\n",
      "[1000]\ttrain-rmse:0.463589\tval-rmse:0.481259\n",
      "[1050]\ttrain-rmse:0.462423\tval-rmse:0.480932\n",
      "[1100]\ttrain-rmse:0.461291\tval-rmse:0.480747\n",
      "[1150]\ttrain-rmse:0.460279\tval-rmse:0.480652\n",
      "[1200]\ttrain-rmse:0.459216\tval-rmse:0.480422\n",
      "[1250]\ttrain-rmse:0.458229\tval-rmse:0.480292\n",
      "[1300]\ttrain-rmse:0.457241\tval-rmse:0.480221\n",
      "[1350]\ttrain-rmse:0.456255\tval-rmse:0.479997\n",
      "[1400]\ttrain-rmse:0.455234\tval-rmse:0.479831\n",
      "[1450]\ttrain-rmse:0.454185\tval-rmse:0.479693\n",
      "[1500]\ttrain-rmse:0.453296\tval-rmse:0.479592\n",
      "[1550]\ttrain-rmse:0.452422\tval-rmse:0.479548\n",
      "[1600]\ttrain-rmse:0.451475\tval-rmse:0.479449\n",
      "[1650]\ttrain-rmse:0.450529\tval-rmse:0.47932\n",
      "[1700]\ttrain-rmse:0.449665\tval-rmse:0.479268\n",
      "[1750]\ttrain-rmse:0.448789\tval-rmse:0.479263\n",
      "Stopping. Best iteration:\n",
      "[1691]\ttrain-rmse:0.449811\tval-rmse:0.479244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.02,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_oth,y_train_oth,test_size=0.2,random_state=2018)\n",
    "\n",
    "y_train_log, y_test_log = np.log1p(y_train), np.log1p(y_test)\n",
    "\n",
    "d_train = xgboost.DMatrix(X_train, y_train_log)\n",
    "d_val=xgboost.DMatrix(X_test, y_test_log)\n",
    "d_test = xgboost.DMatrix(x_test_oth)\n",
    "\n",
    "watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "bst = xgboost.train(params=xgb_params, \n",
    "                    dtrain=d_train, \n",
    "                    num_boost_round=8000, \n",
    "                    early_stopping_rounds=100,\n",
    "                    evals=watchlist, \n",
    "                    verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9399, 72)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_oth_log = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "y_pred_oth = np.expm1(y_pred_oth_log)\n",
    "\n",
    "x_test_oth[\"visitors\"] = y_pred_oth\n",
    "\n",
    "x_test_oth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48261666666666664\n"
     ]
    }
   ],
   "source": [
    "print((0.478342+0.490264+0.479244)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.concat([x_test_iza,x_test_caf,x_test_ita,x_test_oth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 72)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=test_pred.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print('Start...')\n",
    "test['visitors'] = test_pred.visitors\n",
    "sub = test[['id','visitors']].copy()\n",
    "sub.to_csv('submission_rs_recruit_v20_xgb_by_genre_01.csv', index=False)\n",
    "print('Finished.')\n",
    "# xgb\n",
    "# fe\n",
    "# sakata method\n",
    "# LB: ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "\n",
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v20_xgb_by_genre_02.csv', index=False)\n",
    "# xgb\n",
    "# fe\n",
    "# sakata method\n",
    "# weight\n",
    "# LB: 0.481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
